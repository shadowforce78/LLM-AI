import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
import time
import re

def load_model_and_tokenizer(model_path="trained_llm"):
    """Charge le mod√®le et le tokenizer"""
    print("\n" + "="*60)
    print("üöÄ CHARGEMENT DU MOD√àLE")
    print("="*60)
    
    # Rechercher le mod√®le dans diff√©rents emplacements
    model_paths = [model_path, "../models/trained", "../trained_llm", "trained_llm"]
    
    for path in model_paths:
        if os.path.exists(path):
            print(f"Tentative de chargement depuis: {path}")
            try:
                tokenizer = AutoTokenizer.from_pretrained(path)
                model = AutoModelForCausalLM.from_pretrained(path)
                
                # Utiliser CUDA si disponible
                device = "cuda" if torch.cuda.is_available() else "cpu"
                model = model.to(device)
                
                print(f"‚úÖ Mod√®le et tokenizer charg√©s sur {device.upper()}")
                print(f"   Type de mod√®le: {model.config.model_type}")
                print(f"   Nombre de param√®tres: {sum(p.numel() for p in model.parameters()):,}")
                return model, tokenizer, path
            except Exception as e:
                print(f"‚ùå Erreur: {str(e)}")
                continue
    
    # Fallback sur un mod√®le pr√©entra√Æn√©
    print("Tentative avec mod√®le pr√©-entra√Æn√© dbddv01/gpt2-french-small...")
    tokenizer = AutoTokenizer.from_pretrained("dbddv01/gpt2-french-small")
    model = AutoModelForCausalLM.from_pretrained("dbddv01/gpt2-french-small")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    return model, tokenizer, "dbddv01/gpt2-french-small"

def clean_response(text):
    """Nettoie la r√©ponse des r√©f√©rences et √©l√©ments ind√©sirables"""
    # Enlever les r√©f√©rences bibliographiques
    text = re.sub(r'\([^)]*\)', '', text)
    text = re.sub(r'\[[^\]]*\]', '', text)
    text = re.sub(r'ISBN [0-9\-]+', '', text)
    
    # Enlever les sections probl√©matiques
    sections_to_remove = [
        r'Notes et r√©f√©rences.*',
        r'Liens externes.*',
        r'Cat√©gories?:.*',
        r'Portail:.*',
        r'Articles connexes.*',
        r'Bibliographie.*',
        r'Voir aussi.*'
    ]
    
    for pattern in sections_to_remove:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # Nettoyer les caract√®res excessifs
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text

def generate_optimized_response(prompt, model, tokenizer, verbose=True):
    """G√©n√®re une r√©ponse en utilisant la configuration optimale"""
    # Configuration optimale bas√©e sur les tests
    generation_config = {
        "max_length": 100,
        "do_sample": False,  # D√©sactiver l'√©chantillonnage pour plus de d√©terminisme
        "num_beams": 5,      # Beam search pour une meilleure qualit√©
        "early_stopping": True,
        "no_repeat_ngram_size": 2,
        "repetition_penalty": 1.2,
    }
    
    # Format optimal du prompt
    system_context = "R√©ponds de fa√ßon factuelle et pr√©cise √† cette question. "
    
    # # Formater le prompt pour le rendre plus efficace
    # if "capitale" in prompt.lower() and "france" in prompt.lower():
    #     full_prompt = f"{system_context}Question: {prompt} R√©ponse: La capitale de la France est"
    # else:
    full_prompt = f"{system_context}Question: {prompt} R√©ponse:"
    
    # Ajouter le token de d√©but si disponible
    if tokenizer.bos_token:
        full_prompt = f"{tokenizer.bos_token}{full_prompt}"
    
    # Encoder
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    
    # Mesurer le temps
    start_time = time.time()
    
    # G√©n√©rer la r√©ponse
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            **generation_config,
            pad_token_id=tokenizer.eos_token_id if not tokenizer.pad_token_id else tokenizer.pad_token_id
        )
    
    # D√©coder
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extraire et nettoyer
    try:
        if "R√©ponse:" in generated_text:
            response = generated_text.split("R√©ponse:")[-1].strip()
        else:
            response = generated_text.replace(full_prompt, "").strip()
    except Exception:
        response = generated_text.strip()
    
    response = clean_response(response)
    
    # Dur√©e
    elapsed = time.time() - start_time
    
    if verbose:
        print(f"\n‚è±Ô∏è Temps de g√©n√©ration: {elapsed:.2f}s")
    

    
    return response, elapsed

def interactive_mode(model, tokenizer):
    """Mode interactif pour tester le mod√®le"""
    print("\n" + "="*60)
    print("üí¨ MODE INTERACTIF - TESTEUR OPTIMIS√â")
    print("="*60)
    print("Posez vos questions (tapez 'q' pour quitter)")
    
    while True:
        print("\n> ", end="")
        prompt = input().strip()
        
        if prompt.lower() in ['q', 'quit', 'exit']:
            print("Au revoir! üëã")
            break
        
        if not prompt:
            continue
        
        print("\n‚è≥ G√©n√©ration de la r√©ponse...")
        
        try:
            response, elapsed = generate_optimized_response(prompt, model, tokenizer)
            print(f"\nü§ñ R√©ponse ({elapsed:.2f}s):")
            print(response)
        except Exception as e:
            print(f"‚ùå Erreur: {str(e)}")

if __name__ == "__main__":
    # Charger le mod√®le et le tokenizer
    model, tokenizer, model_path = load_model_and_tokenizer()
    
    # D√©finir seed pour la reproductibilit√©
    set_seed(42)
    
    # Lancer le mode interactif
    interactive_mode(model, tokenizer)
