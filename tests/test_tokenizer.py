
import os
from transformers import AutoTokenizer
import torch

def test_tokenizer(model_path="trained_llm"):
    """
    Teste le tokenizer pour v√©rifier son bon fonctionnement et sa coh√©rence.
    """
    print("\n" + "="*50)
    print("üîç TEST DU TOKENIZER")
    print("="*50)
    
    # 1. V√©rifier que le tokenizer peut √™tre charg√©
    print("\n1Ô∏è‚É£ Chargement du tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        print("‚úÖ Tokenizer charg√© avec succ√®s")
        print(f"   Type de tokenizer: {type(tokenizer).__name__}")
        print(f"   Taille du vocabulaire: {len(tokenizer)}")
        print(f"   Tokens sp√©ciaux: {tokenizer.all_special_tokens}")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du tokenizer: {str(e)}")
        return
    
    # 2. Tester les tokens sp√©ciaux
    print("\n2Ô∏è‚É£ V√©rification des tokens sp√©ciaux...")
    required_special_tokens = ['<|bos|>', '<|eos|>', '<|pad|>']
    missing_tokens = [token for token in required_special_tokens if token not in tokenizer.all_special_tokens]
    
    if missing_tokens:
        print(f"‚ö†Ô∏è Tokens sp√©ciaux manquants: {missing_tokens}")
        
        # Ajouter les tokens manquants et sauvegarder
        print("   Tentative d'ajout des tokens manquants...")
        special_tokens_dict = {}
        
        if '<|bos|>' not in tokenizer.all_special_tokens:
            special_tokens_dict['bos_token'] = '<|bos|>'
        if '<|eos|>' not in tokenizer.all_special_tokens:
            special_tokens_dict['eos_token'] = '<|eos|>'
        if '<|pad|>' not in tokenizer.all_special_tokens:
            special_tokens_dict['pad_token'] = '<|pad|>'
            
        tokenizer.add_special_tokens(special_tokens_dict)
        print(f"‚úÖ Tokens sp√©ciaux ajout√©s: {tokenizer.all_special_tokens}")
        
        # Sauvegarder le tokenizer modifi√©
        tokenizer.save_pretrained(model_path)
        print(f"‚úÖ Tokenizer sauvegard√© dans {model_path}")
    else:
        print("‚úÖ Tous les tokens sp√©ciaux sont pr√©sents")
    
    # 3. Tester le tokenizer sur des exemples fran√ßais
    print("\n3Ô∏è‚É£ Test de tokenisation sur des exemples fran√ßais...")
    test_sentences = [
        "La capitale de la France est Paris.",
        "L'intelligence artificielle r√©volutionne le monde.",
        "Comment fonctionnent les r√©seaux de neurones?",
        "Je suis un mod√®le de langage entra√Æn√© en fran√ßais."
    ]
    
    for sentence in test_sentences:
        tokens = tokenizer.tokenize(sentence)
        token_ids = tokenizer.encode(sentence)
        decoded = tokenizer.decode(token_ids)
        
        print(f"\nPhrase: {sentence}")
        print(f"Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        print(f"IDs: {token_ids[:10]}{'...' if len(token_ids) > 10 else ''}")
        print(f"Reconstruction: {decoded}")
        
        if sentence.strip() not in decoded:
            print("‚ö†Ô∏è La reconstruction ne contient pas la phrase originale!")
        else:
            print("‚úÖ Reconstruction correcte")
    
    # 4. Tester la coh√©rence
    print("\n4Ô∏è‚É£ Test de coh√©rence...")
    test_phrase = "La capitale de la France est Paris."
    
    # Tokeniser et d√©coder plusieurs fois pour v√©rifier la coh√©rence
    results = []
    for _ in range(5):
        tokens = tokenizer.encode(test_phrase)
        decoded = tokenizer.decode(tokens)
        results.append((tokens, decoded))
    
    # V√©rifier si tous les r√©sultats sont identiques
    all_tokens_identical = all(result[0] == results[0][0] for result in results)
    all_decoded_identical = all(result[1] == results[0][1] for result in results)
    
    if all_tokens_identical and all_decoded_identical:
        print("‚úÖ Le tokenizer est coh√©rent (donne toujours le m√™me r√©sultat)")
    else:
        print("‚ö†Ô∏è Le tokenizer n'est pas coh√©rent!")
        if not all_tokens_identical:
            print("   Les tokens diff√®rent entre les ex√©cutions")
        if not all_decoded_identical:
            print("   Les d√©codages diff√®rent entre les ex√©cutions")
    
    # 5. Tester avec les tokens sp√©ciaux
    print("\n5Ô∏è‚É£ Test avec tokens sp√©ciaux...")
    
    special_test = f"{tokenizer.bos_token}Question: Quelle est la capitale de la France? R√©ponse:{tokenizer.eos_token}"
    special_tokens = tokenizer.tokenize(special_test)
    special_ids = tokenizer.encode(special_test)
    special_decoded = tokenizer.decode(special_ids)
    
    print(f"Phrase avec tokens sp√©ciaux: {special_test}")
    print(f"Tokens: {special_tokens[:15]}{'...' if len(special_tokens) > 15 else ''}")
    print(f"Reconstruction: {special_decoded}")

    if tokenizer.bos_token in special_decoded and tokenizer.eos_token in special_decoded:
        print("‚úÖ Les tokens sp√©ciaux sont correctement reconnus et reconstruits")
    else:
        print("‚ö†Ô∏è Probl√®me avec les tokens sp√©ciaux dans la reconstruction!")
    
    print("\n" + "="*50)
    print("üèÅ FIN DU TEST")
    print("="*50)

if __name__ == "__main__":
    # Chemin par d√©faut pour le mod√®le
    default_path = "trained_llm"
    
    # V√©rifier si le dossier existe
    if not os.path.exists(default_path):
        print(f"‚ö†Ô∏è Le dossier {default_path} n'existe pas, recherche d'alternatives...")
        
        # Rechercher d'autres emplacements possibles
        alternatives = [
            "models/trained",
            "../trained_llm",
            "output/llm"
        ]
        
        for alt_path in alternatives:
            if os.path.exists(alt_path):
                print(f"‚úÖ Alternative trouv√©e: {alt_path}")
                default_path = alt_path
                break
        else:
            # Si aucune alternative n'est trouv√©e, utiliser un mod√®le pr√©entra√Æn√©
            print("‚ö†Ô∏è Aucun dossier de mod√®le trouv√©, utilisation d'un mod√®le pr√©entra√Æn√©")
            default_path = "dbddv01/gpt2-french-small"
    
    # Lancer le test
    test_tokenizer(default_path)
