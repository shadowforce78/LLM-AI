import torch
import torch.nn as nn
from transformers import PreTrainedModel, GPT2Config, GPT2Model

class CustomGPT(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = GPT2Model(config)  # Mod√®le GPT-2 de base
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # T√™te de pr√©diction
        self.config = config  # Store config for easier access

    def forward(self, input_ids, attention_mask=None):
        # Get outputs from the base model
        outputs = self.model(input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]
        
        # Apply the language model head
        logits = self.lm_head(hidden_states)  # Shape: [batch_size, seq_len, vocab_size]
        
        # Return logits as the main output
        return logits

# üìå Configuration du mod√®le GPT-Medium (~125M params)
if __name__ == "__main__":
    config = GPT2Config(
        vocab_size=32000,  # Taille du vocabulaire (doit correspondre au tokenizer)
        n_embd=768,        # Taille des embeddings (augment√©e de 512 √† 768)
        n_layer=12,        # Nombre de couches (augment√© de 6 √† 12)
        n_head=12,         # Nombre de t√™tes d'attention (augment√© de 8 √† 12)
    )

    # üî• Instanciation du mod√®le
    model = CustomGPT(config)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"‚úÖ Mod√®le GPT-Medium initialis√© avec {total_params:,} param√®tres (~{total_params/1_000_000:.1f}M).")
