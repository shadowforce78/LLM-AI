import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed
import torch.nn as nn
import torch.optim as optim
import time
import random
import re
import numpy as np
from collections import Counter

# Simplification compl√®te de l'assistant pour ne compter que sur le mod√®le entra√Æn√©


def get_model_paths():
    """Retourne une liste des chemins possibles pour le mod√®le, en privil√©giant le dossier principal"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(script_dir)

    # Chemin principal (celui qui doit √™tre privil√©gi√©)
    main_model_path = os.path.join(project_dir, "models", "trained")

    # Autres chemins de fallback, dans l'ordre de pr√©f√©rence
    other_paths = [
        os.path.join(project_dir, "trained_llm"),
        "dbddv01/gpt2-french-small",  # Mod√®le par d√©faut si aucun mod√®le entra√Æn√© n'est trouv√©
    ]

    # On place le chemin principal en premier
    return [main_model_path] + other_paths


def load_model(verbose=True):
    """Charge le mod√®le et le tokenizer depuis les chemins disponibles"""
    if verbose:
        print("‚è≥ Chargement du mod√®le et du tokenizer...")

    model_paths = get_model_paths()
    model = None
    tokenizer = None
    used_path = None

    for path in model_paths:
        if verbose:
            print(f"Tentative de chargement depuis: {path}")
        try:
            # V√©rifier que c'est un dossier qui contient les fichiers n√©cessaires
            if os.path.isdir(path) and any(
                file.endswith(".bin") for file in os.listdir(path)
            ):
                tokenizer = AutoTokenizer.from_pretrained(path)
                model = AutoModelForCausalLM.from_pretrained(path)
                used_path = path
                break
            elif not os.path.isdir(path):  # Si c'est un mod√®le HuggingFace Hub
                tokenizer = AutoTokenizer.from_pretrained(path)
                model = AutoModelForCausalLM.from_pretrained(path)
                used_path = path
                break
        except (OSError, ValueError, FileNotFoundError) as e:
            if verbose:
                print(f"√âchec: {str(e)}")
            continue

    # Si on n'a pas trouv√© de mod√®le principal, alors seulement chercher dans les sous-dossiers
    if model is None:
        if verbose:
            print(
                "Aucun mod√®le trouv√© dans le dossier principal, recherche dans les sous-dossiers..."
            )

        # Chemin principal pour chercher les best_model
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(script_dir)
        main_model_dir = os.path.join(project_dir, "models", "trained")

        if os.path.isdir(main_model_dir):
            import glob

            # Chercher les dossiers best_model_* et les trier par score
            best_model_pattern = os.path.join(main_model_dir, "best_model_*")
            best_model_paths = sorted(
                glob.glob(best_model_pattern),
                key=lambda x: float(x.split("_")[-1]),
                reverse=False,
            )  # Du meilleur (plus bas) au pire

            for path in best_model_paths:
                if verbose:
                    print(f"Tentative de chargement depuis (sous-dossier): {path}")
                try:
                    tokenizer = AutoTokenizer.from_pretrained(path)
                    model = AutoModelForCausalLM.from_pretrained(path)
                    used_path = path
                    break
                except (OSError, ValueError) as e:
                    if verbose:
                        print(f"√âchec: {str(e)}")
                    continue

    if model is None:
        raise ValueError(
            "Impossible de charger le mod√®le depuis les chemins disponibles. "
            "Assurez-vous que le dossier 'models/trained' contient un mod√®le valide."
        )

    # D√©placer le mod√®le sur GPU si disponible
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)

    if verbose:
        print(f"‚úÖ Mod√®le charg√© depuis {used_path} sur {device.upper()}")

    return model, tokenizer


class QuestionAnalyzer(nn.Module):
    """Petit r√©seau de neurones pour analyser les questions et identifier leur type"""

    def __init__(self, vocab_size=5000, embedding_dim=64, hidden_dim=32, output_dim=6):
        super(QuestionAnalyzer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, batch_first=True, bidirectional=True
        )
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # bidirectional -> *2
        self.softmax = nn.Softmax(dim=1)

        # Cat√©gories de questions que le r√©seau peut identifier
        self.categories = [
            "factual",
            "opinion",
            "how-to",
            "description",
            "explanation",
            "other",
        ]

        # Initialisation explicite avec des biais appropri√©s
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialise les poids du r√©seau avec des biais pour les types de questions courants"""
        # Assurer une meilleure d√©tection des questions factuelles vs opinions
        torch.nn.init.xavier_uniform_(self.fc.weight)
        # Bias factuel plus √©lev√© pour reconna√Ætre les questions factuelles
        self.fc.bias.data = torch.tensor(
            [0.5, -0.3, -0.2, -0.2, -0.2, -0.1], dtype=torch.float
        )

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        # Prendre uniquement la derni√®re sortie pour la classification
        lstm_out = lstm_out[:, -1, :]
        out = self.fc(lstm_out)
        return self.softmax(out)

    def predict(self, question, tokenizer):
        """Pr√©dit la cat√©gorie d'une question avec une analyse plus robuste"""
        # Normaliser la question
        question = question.lower().strip()

        # D√©tection directe pour les questions factuelles courantes
        factual_patterns = [
            r"quelle\s+(est|sont)",
            r"qu'est(\s+|-)+ce que",
            r"(o√π|quand|qui|pourquoi|comment|combien)",
            r"(capitale|superficie|population|nombre|taille|date|couleur)",
        ]

        for pattern in factual_patterns:
            if re.search(pattern, question):
                return "factual", 0.9

        # Suite du processus normal pour les cas ambigus
        words = question.split()
        # Tokenisation simplifi√©e
        word_to_idx = {word: min(hash(word) % 4999, 4999) for word in words}
        # Convertir la question en indices
        indices = [word_to_idx.get(word, 4999) for word in words]
        # Padding/truncation
        if len(indices) > 30:
            indices = indices[:30]
        else:
            indices = indices + [0] * (30 - len(indices))

        # Convertir en tensor et pr√©dire
        tensor = torch.tensor([indices], dtype=torch.long)
        with torch.no_grad():
            outputs = self(tensor)
            _, predicted = torch.max(outputs, 1)

        # Retourner la cat√©gorie et la confiance
        category = self.categories[predicted.item()]
        confidence = outputs[0][predicted.item()].item()
        return category, confidence


def create_or_load_analyzer():
    """Cr√©e ou charge le r√©seau de neurones d'analyse de questions"""
    try:
        # Tenter de charger un mod√®le existant
        analyzer = QuestionAnalyzer()
        model_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "question_analyzer.pt"
        )
        if os.path.exists(model_path):
            analyzer.load_state_dict(torch.load(model_path))
            print("R√©seau d'analyse de questions charg√© avec succ√®s")
        else:
            print("Cr√©ation d'un nouveau r√©seau d'analyse de questions")
            # Initialisation avec quelques exemples pr√©-entra√Æn√©s
            pretrain_analyzer(analyzer)
        return analyzer
    except Exception as e:
        print(f"Erreur lors du chargement du r√©seau d'analyse: {str(e)}")
        # Cr√©er un nouveau mod√®le en cas d'erreur
        return QuestionAnalyzer()


def pretrain_analyzer(analyzer):
    """Pr√©-entra√Æne le r√©seau avec quelques exemples repr√©sentatifs"""
    # R√®gles sp√©cifiques pour l'embedding des mots cl√©s
    factual_keywords = [
        "quelle",
        "qui",
        "quand",
        "o√π",
        "combien",
        "pourquoi",
        "capitale",
        "population",
        "superficie",
        "pr√©sident",
        "nombre",
        "habitant",
        "est",
    ]
    opinion_keywords = [
        "penses",
        "crois",
        "avis",
        "opinion",
        "pr√©f√®res",
        "meilleur",
        "pire",
    ]
    howto_keywords = [
        "comment",
        "proc√©dure",
        "√©tapes",
        "faire",
        "cr√©er",
        "d√©velopper",
        "fabriquer",
    ]

    # Cr√©er un dictionnaire word_to_idx fictif pour les mots cl√©s importants
    word_to_idx = {}
    for i, word in enumerate(factual_keywords):
        word_to_idx[word] = i + 10  # Commencer √† 10

    for i, word in enumerate(opinion_keywords):
        word_to_idx[word] = i + 100  # Opinion words start at index 100

    for i, word in enumerate(howto_keywords):
        word_to_idx[word] = i + 200  # How-to words start at index 200

    # Initialiser les embeddings pour les mots cl√©s
    for word, idx in word_to_idx.items():
        if idx < 100:  # Factual words
            # Biais vers la cat√©gorie factuelle (indice 0)
            analyzer.embedding.weight.data[idx] = torch.tensor(
                [
                    0.5 if j == 20 else 0.1 * random.random()
                    for j in range(analyzer.embedding.weight.data[idx].shape[0])
                ],
                dtype=torch.float,
            )
        elif idx < 200:  # Opinion words
            # Biais vers la cat√©gorie opinion (indice 1)
            analyzer.embedding.weight.data[idx] = torch.tensor(
                [
                    0.5 if j == 10 else 0.1 * random.random()
                    for j in range(analyzer.embedding.weight.data[idx].shape[0])
                ],
                dtype=torch.float,
            )
        else:  # How-to words
            # Biais vers la cat√©gorie how-to (indice 2)
            analyzer.embedding.weight.data[idx] = torch.tensor(
                [
                    0.5 if j == 30 else 0.1 * random.random()
                    for j in range(analyzer.embedding.weight.data[idx].shape[0])
                ],
                dtype=torch.float,
            )


def generate_response(
    prompt,
    model,
    tokenizer,
    analyzer,
    max_length=150,  # R√©duit pour des r√©ponses plus concises
    temperature=0.3,  # Temp√©rature r√©duite pour plus de focus
    top_p=0.85,
    repetition_penalty=1.2,
):
    """G√©n√®re une r√©ponse bas√©e uniquement sur le mod√®le entra√Æn√©"""
    # Analyser la question avec le r√©seau de neurones
    question_type, confidence = analyzer.predict(prompt, tokenizer)

    # Am√©liorer le prompt avec plus de structure et de contexte

    context = "R√©ponds de fa√ßon factuelle et concise √† cette question. "

    prompt_text = prompt.strip()
    suffix = "\nR√©ponse: "

    # Construire le prompt final
    full_prompt = f"{context}{prompt_text}{suffix}"

    # Tokenize et convertir en tensor
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    # Param√®tres de g√©n√©ration ajust√©s pour plus de pr√©cision
    gen_kwargs = {
        "max_length": max_length + len(inputs["input_ids"][0]),
        "temperature": temperature,
        "do_sample": True,
        "top_p": top_p,
        "top_k": 20,  # R√©duit pour plus de pr√©cision
        "repetition_penalty": repetition_penalty,
        "no_repeat_ngram_size": 3,
        "num_beams": 3,
        "early_stopping": True,
    }

    # G√©n√©rer la r√©ponse avec le mod√®le
    set_seed(42)  # Seed fixe pour la coh√©rence
    output_sequences = model.generate(**inputs, **gen_kwargs)

    # D√©coder et extraire la r√©ponse
    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)

    try:
        # Extraire uniquement la partie r√©ponse
        response = generated_text.split(suffix)[-1].strip()
    except:
        response = generated_text.strip()

    # Nettoyage am√©lior√© de la r√©ponse
    response = clean_response(response, prompt)

    return response, question_type


def clean_response(response, original_question):
    """Nettoie la r√©ponse des artefacts avec un nettoyage plus agressif"""

    # √âliminer les r√©f√©rences bibliographiques et citations
    response = re.sub(r"\([^)]*\)", "", response)
    response = re.sub(r"\[[^\]]*\]", "", response)
    response = re.sub(r"ISBN [0-9\-]+", "", response)

    # √âliminer les phrases avec des mots cl√©s probl√©matiques
    problem_keywords = [
        "Naissances",
        "Portail",
        "Cat√©gorie:",
        "Notes",
        "R√©f√©rences",
        "pr√©sentation en ligne",
        "Jean-Pierre",
        "Jean-Paul",
        "coll.",
        "Presses universitaires",
    ]

    # Diviser en phrases
    sentences = re.split(r"([.!?]\s+)", response)
    cleaned_sentences = []
    current_sentence = ""

    for i in range(0, len(sentences)):
        if i % 2 == 0:  # Partie de texte
            current_sentence = sentences[i]
        else:  # Ponctuation
            current_sentence += sentences[i]

            # V√©rifier si la phrase contient des mots probl√©matiques
            if not any(keyword in current_sentence for keyword in problem_keywords):
                cleaned_sentences.append(current_sentence)

    cleaned_response = "".join(cleaned_sentences)

    # Nettoyage suppl√©mentaire
    cleaned_response = re.sub(r"\[\d+\]", "", cleaned_response)
    cleaned_response = re.sub(r"\n+", " ", cleaned_response)
    cleaned_response = re.sub(r" +", " ", cleaned_response)

    # Si la r√©ponse est trop courte ou vide apr√®s nettoyage, donner une r√©ponse par d√©faut
    if len(cleaned_response.strip()) < 10:
        return "Je n'ai pas assez d'informations pour r√©pondre pr√©cis√©ment √† cette question."

    return cleaned_response.strip()


def main():
    """Point d'entr√©e principal de l'assistant"""
    # Charger le mod√®le et le tokenizer
    model, tokenizer = load_model()

    # Charger le r√©seau d'analyse de questions
    analyzer = create_or_load_analyzer()

    # Affichage d'en-t√™te
    print("\nüí¨ Assistant IA Fran√ßais")
    print("=" * 50)
    print("üìå COMMANDES:")
    print(" - Tapez votre question et appuyez sur Entr√©e")
    print(" - Tapez 'q' pour quitter")
    print("=" * 50)

    # Boucle d'interaction simplifi√©e
    while True:
        print("\n‚û§ ", end="")
        user_input = input().strip()

        if user_input.lower() in ["q", "quit", "exit"]:
            break

        if not user_input:
            continue

        print("\nüß† G√©n√©ration de la r√©ponse...")
        start_time = time.time()

        try:
            response, question_type = generate_response(
                user_input,
                model,
                tokenizer,
                analyzer,
                max_length=250,
                temperature=0.85,
            )

            elapsed_time = time.time() - start_time
            print(f"\nü§ñ R√©ponse (temps: {elapsed_time:.2f}s):")
            print(response)
            print("\n" + "=" * 50)

        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
            import traceback

            traceback.print_exc()

    print("\nAu revoir! üëã")


if __name__ == "__main__":
    main()
