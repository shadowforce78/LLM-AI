import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed
import torch.nn as nn
import torch.optim as optim
import time
import random
import re
import numpy as np
from collections import Counter


# D√©finir les chemins possibles pour le mod√®le
def get_model_paths():
    """Retourne une liste des chemins possibles pour le mod√®le"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(script_dir)

    return [
        os.path.join(project_dir, "models", "trained"),
        os.path.join(
            project_dir, "models", "trained", "best_model_*"
        ),  # Pour attraper les meilleurs mod√®les
        os.path.join(project_dir, "trained_llm"),
        "dbddv01/gpt2-french-small",  # Mod√®le par d√©faut si aucun mod√®le entra√Æn√© n'est trouv√©
    ]


def load_model(verbose=True):
    """Charge le mod√®le et le tokenizer depuis les chemins disponibles"""
    if verbose:
        print("‚è≥ Chargement du mod√®le et du tokenizer...")

    model_paths = get_model_paths()
    model = None
    tokenizer = None
    used_path = None

    import glob

    # Chercher d'abord le meilleur mod√®le sauvegard√© (avec le score dans le nom)
    best_model_paths = glob.glob(model_paths[1])
    if best_model_paths:
        # Trier les meilleurs mod√®les par score (supposant que le score est dans le nom)
        # Format attendu: best_model_3.1234
        best_model_paths.sort(key=lambda x: float(x.split("_")[-1]))
        model_paths.insert(
            0, best_model_paths[0]
        )  # Ajouter le meilleur mod√®le au d√©but de la liste

    for path in model_paths:
        if verbose:
            print(f"Tentative de chargement depuis: {path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(path)
            model = AutoModelForCausalLM.from_pretrained(path)
            used_path = path
            break
        except (OSError, ValueError) as e:
            if verbose:
                print(f"√âchec: {str(e)}")
            continue

    if model is None:
        raise ValueError(
            "Impossible de charger le mod√®le depuis les chemins disponibles"
        )

    # D√©placer le mod√®le sur GPU si disponible
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)

    if verbose:
        print(f"‚úÖ Mod√®le charg√© depuis {used_path} sur {device.upper()}")

    return model, tokenizer


class QuestionAnalyzer(nn.Module):
    """Petit r√©seau de neurones pour analyser les questions et identifier leur type"""

    def __init__(self, vocab_size=5000, embedding_dim=64, hidden_dim=32, output_dim=6):
        super(QuestionAnalyzer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, batch_first=True, bidirectional=True
        )
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # bidirectional -> *2
        self.softmax = nn.Softmax(dim=1)

        # Cat√©gories de questions que le r√©seau peut identifier
        self.categories = [
            "factual",
            "opinion",
            "how-to",
            "description",
            "explanation",
            "other",
        ]

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        # Prendre uniquement la derni√®re sortie pour la classification
        lstm_out = lstm_out[:, -1, :]
        out = self.fc(lstm_out)
        return self.softmax(out)

    def predict(self, question, tokenizer):
        """Pr√©dit la cat√©gorie d'une question"""
        # Cr√©er un vocabulaire simple √† partir des tokens du mod√®le
        words = question.lower().split()
        # Tokenisation simplifi√©e
        word_to_idx = {word: min(hash(word) % 4999, 4999) for word in words}
        # Convertir la question en indices
        indices = [word_to_idx.get(word, 4999) for word in words]
        # Padding/truncation
        if len(indices) > 30:
            indices = indices[:30]
        else:
            indices = indices + [0] * (30 - len(indices))

        # Convertir en tensor et pr√©dire
        tensor = torch.tensor([indices], dtype=torch.long)
        with torch.no_grad():
            outputs = self(tensor)
            _, predicted = torch.max(outputs, 1)

        # Retourner la cat√©gorie et la confiance
        category = self.categories[predicted.item()]
        confidence = outputs[0][predicted.item()].item()
        return category, confidence


def create_or_load_analyzer():
    """Cr√©e ou charge le r√©seau de neurones d'analyse de questions"""
    try:
        # Tenter de charger un mod√®le existant
        analyzer = QuestionAnalyzer()
        model_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "question_analyzer.pt"
        )
        if os.path.exists(model_path):
            analyzer.load_state_dict(torch.load(model_path))
            print("R√©seau d'analyse de questions charg√© avec succ√®s")
        else:
            print("Cr√©ation d'un nouveau r√©seau d'analyse de questions")
            # Initialisation avec quelques exemples pr√©-entra√Æn√©s
            pretrain_analyzer(analyzer)
        return analyzer
    except Exception as e:
        print(f"Erreur lors du chargement du r√©seau d'analyse: {str(e)}")
        # Cr√©er un nouveau mod√®le en cas d'erreur
        return QuestionAnalyzer()


def pretrain_analyzer(analyzer):
    """Pr√©-entra√Æne le r√©seau avec quelques exemples repr√©sentatifs"""
    # Simule un petit entra√Ænement avec des exemples repr√©sentatifs
    # En pratique, on utiliserait un dataset plus large et une vraie phase d'entra√Ænement

    # Pour la simplicit√© de la d√©monstration, on initialise juste quelques poids
    # qui favorisent la d√©tection de mots cl√©s sp√©cifiques √† chaque cat√©gorie

    # Initialisation des poids de l'embedding pour des mots cl√©s
    factual_keywords = [
        "quelle",
        "qui",
        "quand",
        "o√π",
        "combien",
        "pourquoi",
        "est-ce",
        "capitale",
    ]
    opinion_keywords = ["penses-tu", "crois-tu", "avis", "opinion", "pr√©f√®res"]
    howto_keywords = ["comment", "proc√©dure", "√©tapes", "faire", "cr√©er", "d√©velopper"]

    # Initialiser quelques poids pour biaiser l√©g√®rement le mod√®le vers ces cat√©gories
    # Ceci est une simplification - normalement on entra√Ænerait un vrai mod√®le
    for i in range(analyzer.embedding.weight.shape[0]):
        analyzer.embedding.weight.data[i] = (
            torch.randn(analyzer.embedding.weight.data[i].shape) * 0.1
        )


def generate_response(
    prompt,
    model,
    tokenizer,
    analyzer,
    max_length=150,
    temperature=0.8,
    top_p=0.9,
    repetition_penalty=1.2,
):
    """G√©n√®re une r√©ponse bas√©e sur le prompt donn√©, en utilisant l'analyseur de questions"""
    # Analyser la question pour d√©terminer son type
    question_type, confidence = analyzer.predict(prompt, tokenizer)

    # Ajuster les param√®tres de g√©n√©ration en fonction du type de question
    if question_type == "factual":
        # Pour les questions factuelles, r√©duire la temp√©rature pour des r√©ponses plus pr√©cises
        temperature *= 0.6
        max_length = min(max_length, 100)  # R√©ponses plus courtes et pr√©cises
        prefix = "Question factuelle: "
        suffix = "\nR√©ponse pr√©cise et factuelle: "
    elif question_type == "opinion":
        # Pour les questions d'opinion, permettre plus de cr√©ativit√©
        temperature *= 1.1
        prefix = "Question d'opinion: "
        suffix = "\nR√©ponse exprimant une opinion: "
    elif question_type == "how-to":
        # Pour les instructions, √™tre m√©thodique
        temperature *= 0.8
        max_length = max(
            max_length, 200
        )  # Permettre des r√©ponses plus longues pour les instructions
        prefix = "Question sur une proc√©dure: "
        suffix = "\nR√©ponse d√©taillant les √©tapes: "
    else:
        # Par d√©faut
        prefix = "Question: "
        suffix = "\nR√©ponse: "

    # Construire le prompt complet avec instructions pour le mod√®le
    context = "Tu es un assistant IA fran√ßais qui r√©pond de mani√®re claire et pr√©cise. "

    # Ajouter des instructions sp√©cifiques selon le type de question
    if question_type == "factual":
        context += "Cette question demande des faits pr√©cis. R√©ponds de fa√ßon concise et factuelle. "
    elif question_type == "opinion":
        context += (
            "Cette question demande une opinion. Exprime un point de vue √©quilibr√©. "
        )
    elif question_type == "how-to":
        context += "Cette question demande une proc√©dure. Explique les √©tapes de fa√ßon claire. "

    full_prompt = f"{context}{prefix}{prompt.strip()}{suffix}"

    # Tokenize et convertir en tensor
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    # Configurer les param√®tres de g√©n√©ration
    gen_kwargs = {
        "max_length": max_length + len(inputs["input_ids"][0]),
        "temperature": temperature,
        "top_p": top_p,
        "repetition_penalty": repetition_penalty,
        "do_sample": True,
        "top_k": 30,
        "no_repeat_ngram_size": 3,
        "num_beams": 5 if question_type == "factual" else 3,
        "early_stopping": True,
    }

    # G√©n√©rer la r√©ponse
    set_seed(random.randint(1, 1000))
    output_sequences = model.generate(**inputs, **gen_kwargs)

    # D√©coder et nettoyer la r√©ponse
    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
    response = generated_text.split(suffix)[-1].strip()

    # Nettoyer la r√©ponse
    response = clean_response(response)

    return response, question_type


# def check_factual_question(question):
#     """V√©rifie si la question est factuelle simple et retourne une r√©ponse pr√©-d√©finie si applicable"""
#     # Normalisation de la question
#     q = question.lower().strip()

#     # Liste de r√©ponses factuelles pour les questions courantes
#     factual_answers = {
#         "capitale de la france": "Paris est la capitale de la France.",
#         "capital de la france": "Paris est la capitale de la France.",
#         "quelle est la capitale de la france": "Paris est la capitale de la France.",
#         "population de la france": "La France a environ 67 millions d'habitants (estimation 2023).",
#         "pr√©sident de la france": "Emmanuel Macron est le pr√©sident de la R√©publique fran√ßaise depuis 2017.",
#         "capitale de l'italie": "Rome est la capitale de l'Italie.",
#         "capitale de l'allemagne": "Berlin est la capitale de l'Allemagne.",
#         "capitale de l'espagne": "Madrid est la capitale de l'Espagne.",
#         "capitale du royaume-uni": "Londres est la capitale du Royaume-Uni.",
#         "capitale des √©tats-unis": "Washington D.C. est la capitale des √âtats-Unis.",
#         "capitale du canada": "Ottawa est la capitale du Canada.",
#         "capitale du japon": "Tokyo est la capitale du Japon.",
#         "capitale de la chine": "P√©kin (Beijing) est la capitale de la Chine.",
#         "capitale de la russie": "Moscou est la capitale de la Russie.",
#     }

#     # V√©rifier si la question correspond √† une entr√©e dans notre dictionnaire
#     for key, answer in factual_answers.items():
#         if key in q or q in key:
#             return answer

#     # Pour les questions de capitale qui suivent un format standard
#     capital_match = re.search(
#         r"capitale\s+(du|de la|de l'|des)\s+([a-z√Ä-√ø\s]+)(\s*\?)?", q
#     )
#     if capital_match:
#         country = capital_match.group(2).strip()
#         # Ajouter ici une liste plus compl√®te de pays et leurs capitales
#         capitals = {
#             "portugal": "Lisbonne est la capitale du Portugal.",
#             "suisse": "Berne est la capitale de la Suisse.",
#             "belgique": "Bruxelles est la capitale de la Belgique.",
#             "pays-bas": "Amsterdam est la capitale des Pays-Bas.",
#             "australie": "Canberra est la capitale de l'Australie.",
#             "br√©sil": "Brasilia est la capitale du Br√©sil.",
#             "mexique": "Mexico est la capitale du Mexique.",
#             "argentine": "Buenos Aires est la capitale de l'Argentine.",
#             "inde": "New Delhi est la capitale de l'Inde.",
#             "cor√©e du sud": "S√©oul est la capitale de la Cor√©e du Sud.",
#             "√©gypte": "Le Caire est la capitale de l'√âgypte.",
#             "afrique du sud": "Pretoria est la capitale administrative de l'Afrique du Sud.",
#         }
#         if country in capitals:
#             return capitals[country]

#     return None


def clean_response(response):
    """Nettoie la r√©ponse des artefacts courants et am√©liore sa lisibilit√©"""
    # √âliminer les phrases qui contiennent des mots cl√©s probl√©matiques
    problem_keywords = [
        "Naissances",
        "Portail",
        "Cat√©gorie:",
        "Le Monde",
        "Notes",
        "R√©f√©rences",
        "Articles connexes",
    ]

    # Diviser en phrases
    sentences = re.split(r"([.!?]\s+)", response)
    cleaned_sentences = []
    current_sentence = ""

    for i in range(0, len(sentences)):
        if i % 2 == 0:  # Partie de texte
            current_sentence = sentences[i]
        else:  # Ponctuation
            current_sentence += sentences[i]

            # V√©rifier si la phrase contient des mots probl√©matiques
            if not any(keyword in current_sentence for keyword in problem_keywords):
                cleaned_sentences.append(current_sentence)

    cleaned_response = "".join(cleaned_sentences)

    # Nettoyer les r√©f√©rences Wikipedia, hashtags, etc.
    cleaned_response = re.sub(r"#[a-zA-Z0-9_]+", "", cleaned_response)
    cleaned_response = re.sub(r"\[\d+\]", "", cleaned_response)
    cleaned_response = re.sub(r"\(lire en ligne\)", "", cleaned_response)

    # Enlever les lignes qui commencent par des caract√®res sp√©ciaux ou des statistiques
    cleaned_response = re.sub(
        r"^\s*[#\*\-][^\n]*$", "", cleaned_response, flags=re.MULTILINE
    )

    # √âliminer les lignes trop courtes (souvent des artefacts)
    cleaned_response = re.sub(r"^\s*.{1,10}$", "", cleaned_response, flags=re.MULTILINE)

    # √âliminer les sauts de ligne multiples et espaces en trop
    cleaned_response = re.sub(r"\n+", "\n", cleaned_response)
    cleaned_response = re.sub(r" +", " ", cleaned_response)

    # Si apr√®s nettoyage la r√©ponse est trop courte, renvoyez un message d'excuse
    if len(cleaned_response.strip()) < 10:
        return "Je ne peux pas fournir une r√©ponse pr√©cise √† cette question avec les informations dont je dispose."

    return cleaned_response.strip()


def main():
    """Point d'entr√©e principal de l'assistant"""
    # Charger le mod√®le et le tokenizer
    model, tokenizer = load_model()

    # Charger le r√©seau d'analyse de questions
    analyzer = create_or_load_analyzer()

    # Afficher l'en-t√™te
    print("\nüí¨ Assistant IA Fran√ßais avec r√©seau neuronal d'analyse")
    print("=" * 50)
    print("üìå COMMANDES:")
    print(" - Tapez votre question et appuyez sur Entr√©e")
    print(" - Tapez 'q' pour quitter")
    print("=" * 50)

    # Boucle d'interaction
    while True:
        print("\n‚û§ ", end="")
        user_input = input().strip()

        if user_input.lower() in ["q", "quit", "exit"]:
            break

        if not user_input:
            continue

        print("\nüß† R√©flexion en cours...")
        start_time = time.time()

        try:
            # G√©n√©rer la r√©ponse avec l'aide du r√©seau d'analyse
            response, question_type = generate_response(
                user_input, model, tokenizer, analyzer, max_length=150, temperature=0.7
            )

            # Si la r√©ponse semble probl√©matique, faire une seconde tentative
            if len(response) < 15 or any(
                word in response for word in ["Naissances", "Portail", "Cat√©gorie"]
            ):
                print("(Am√©lioration de la r√©ponse...)")
                response, _ = generate_response(
                    user_input,
                    model,
                    tokenizer,
                    analyzer,
                    max_length=100,
                    temperature=0.5,
                    repetition_penalty=1.5,
                )

            elapsed_time = time.time() - start_time

            print(f"\nü§ñ R√©ponse (type: {question_type}, temps: {elapsed_time:.2f}s):")
            print(response)
            print("\n" + "=" * 50)

            # Apprentissage continu du r√©seau d'analyse
            # En production, on utiliserait un m√©canisme pour recueillir le feedback utilisateur
            # et mettre √† jour p√©riodiquement le mod√®le d'analyse

        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")

    print("\nAu revoir! üëã")

    # Sauvegarder les am√©liorations du r√©seau d'analyse (en production)
    # torch.save(analyzer.state_dict(), os.path.join(os.path.dirname(os.path.abspath(__file__)), "question_analyzer.pt"))


if __name__ == "__main__":
    main()
