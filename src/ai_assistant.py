import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed
import torch.nn as nn
import torch.optim as optim
import time
import random
import re
import numpy as np
from collections import Counter

# Dictionnaire de r√©ponses pr√©d√©finies pour les questions les plus courantes
FACTUAL_ANSWERS = {
    "capitale france": "La capitale de la France est Paris.",
    "capitale italie": "La capitale de l'Italie est Rome.",
    "capitale allemagne": "La capitale de l'Allemagne est Berlin.",
    "capitale espagne": "La capitale de l'Espagne est Madrid.",
    "capitale royaume-uni": "La capitale du Royaume-Uni est Londres.",
    "capitale angleterre": "Londres est la capitale de l'Angleterre.",
    "capitale etats-unis": "La capitale des √âtats-Unis est Washington D.C.",
    "capitale usa": "La capitale des √âtats-Unis est Washington D.C.",
    "capitale canada": "La capitale du Canada est Ottawa.",
    "capitale japon": "La capitale du Japon est Tokyo.",
    "capitale chine": "La capitale de la Chine est P√©kin (Beijing).",
    "capitale russie": "La capitale de la Russie est Moscou.",
    "population france": "La France compte environ 67 millions d'habitants (estimation 2023).",
    "president france": "Le pr√©sident de la R√©publique fran√ßaise est Emmanuel Macron depuis 2017.",
    "langue france": "La langue officielle de la France est le fran√ßais.",
    "monnaie france": "La monnaie de la France est l'euro (‚Ç¨).",
}


def get_factual_answer(question):
    """V√©rifie si la question correspond √† une r√©ponse factuelle pr√©d√©finie"""
    # Normaliser la question: retirer ponctuation et mettre en minuscules
    normalized = question.lower()
    normalized = re.sub(r"[^\w\s]", "", normalized)
    normalized = re.sub(r"\s+", " ", normalized).strip()

    # Recherche exacte
    for key, answer in FACTUAL_ANSWERS.items():
        search_patterns = [
            f"quelle est la {key}",
            f"quel est le {key}",
            f"qui est le {key}",
            f"{key} est",
            f"{key}",
        ]
        if any(pattern in normalized for pattern in search_patterns):
            return answer

    # Recherche plus complexe pour les capitales
    if "capitale" in normalized:
        for key, answer in FACTUAL_ANSWERS.items():
            if "capitale" in key and key.split()[1] in normalized:
                return answer

    return None


# D√©finir les chemins possibles pour le mod√®le
def get_model_paths():
    """Retourne une liste des chemins possibles pour le mod√®le"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(script_dir)

    return [
        os.path.join(project_dir, "models", "trained"),
        os.path.join(
            project_dir, "models", "trained", "best_model_*"
        ),  # Pour attraper les meilleurs mod√®les
        os.path.join(project_dir, "trained_llm"),
        "dbddv01/gpt2-french-small",  # Mod√®le par d√©faut si aucun mod√®le entra√Æn√© n'est trouv√©
    ]


def load_model(verbose=True):
    """Charge le mod√®le et le tokenizer depuis les chemins disponibles"""
    if verbose:
        print("‚è≥ Chargement du mod√®le et du tokenizer...")

    model_paths = get_model_paths()
    model = None
    tokenizer = None
    used_path = None

    import glob

    # Chercher d'abord le meilleur mod√®le sauvegard√© (avec le score dans le nom)
    best_model_paths = glob.glob(model_paths[1])
    if best_model_paths:
        # Trier les meilleurs mod√®les par score (supposant que le score est dans le nom)
        # Format attendu: best_model_3.1234
        best_model_paths.sort(key=lambda x: float(x.split("_")[-1]))
        model_paths.insert(
            0, best_model_paths[0]
        )  # Ajouter le meilleur mod√®le au d√©but de la liste

    for path in model_paths:
        if verbose:
            print(f"Tentative de chargement depuis: {path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(path)
            model = AutoModelForCausalLM.from_pretrained(path)
            used_path = path
            break
        except (OSError, ValueError) as e:
            if verbose:
                print(f"√âchec: {str(e)}")
            continue

    if model is None:
        raise ValueError(
            "Impossible de charger le mod√®le depuis les chemins disponibles"
        )

    # D√©placer le mod√®le sur GPU si disponible
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)

    if verbose:
        print(f"‚úÖ Mod√®le charg√© depuis {used_path} sur {device.upper()}")

    return model, tokenizer


class QuestionAnalyzer(nn.Module):
    """Petit r√©seau de neurones pour analyser les questions et identifier leur type"""

    def __init__(self, vocab_size=5000, embedding_dim=64, hidden_dim=32, output_dim=6):
        super(QuestionAnalyzer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, batch_first=True, bidirectional=True
        )
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # bidirectional -> *2
        self.softmax = nn.Softmax(dim=1)

        # Cat√©gories de questions que le r√©seau peut identifier
        self.categories = [
            "factual",
            "opinion",
            "how-to",
            "description",
            "explanation",
            "other",
        ]

        # Initialisation explicite avec des biais appropri√©s
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialise les poids du r√©seau avec des biais pour les types de questions courants"""
        # Assurer une meilleure d√©tection des questions factuelles vs opinions
        torch.nn.init.xavier_uniform_(self.fc.weight)
        # Bias factuel plus √©lev√© pour reconna√Ætre les questions factuelles
        self.fc.bias.data = torch.tensor(
            [0.5, -0.3, -0.2, -0.2, -0.2, -0.1], dtype=torch.float
        )

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        # Prendre uniquement la derni√®re sortie pour la classification
        lstm_out = lstm_out[:, -1, :]
        out = self.fc(lstm_out)
        return self.softmax(out)

    def predict(self, question, tokenizer):
        """Pr√©dit la cat√©gorie d'une question avec une analyse plus robuste"""
        # Normaliser la question
        question = question.lower().strip()

        # D√©tection directe pour les questions factuelles courantes
        factual_patterns = [
            r"quelle\s+(est|sont)",
            r"qu'est(\s+|-)+ce que",
            r"(o√π|quand|qui|pourquoi|comment|combien)",
            r"(capitale|superficie|population|nombre|taille|date|couleur)",
        ]

        for pattern in factual_patterns:
            if re.search(pattern, question):
                return "factual", 0.9

        # Suite du processus normal pour les cas ambigus
        words = question.split()
        # Tokenisation simplifi√©e
        word_to_idx = {word: min(hash(word) % 4999, 4999) for word in words}
        # Convertir la question en indices
        indices = [word_to_idx.get(word, 4999) for word in words]
        # Padding/truncation
        if len(indices) > 30:
            indices = indices[:30]
        else:
            indices = indices + [0] * (30 - len(indices))

        # Convertir en tensor et pr√©dire
        tensor = torch.tensor([indices], dtype=torch.long)
        with torch.no_grad():
            outputs = self(tensor)
            _, predicted = torch.max(outputs, 1)

        # Retourner la cat√©gorie et la confiance
        category = self.categories[predicted.item()]
        confidence = outputs[0][predicted.item()].item()
        return category, confidence


def create_or_load_analyzer():
    """Cr√©e ou charge le r√©seau de neurones d'analyse de questions"""
    try:
        # Tenter de charger un mod√®le existant
        analyzer = QuestionAnalyzer()
        model_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "question_analyzer.pt"
        )
        if os.path.exists(model_path):
            analyzer.load_state_dict(torch.load(model_path))
            print("R√©seau d'analyse de questions charg√© avec succ√®s")
        else:
            print("Cr√©ation d'un nouveau r√©seau d'analyse de questions")
            # Initialisation avec quelques exemples pr√©-entra√Æn√©s
            pretrain_analyzer(analyzer)
        return analyzer
    except Exception as e:
        print(f"Erreur lors du chargement du r√©seau d'analyse: {str(e)}")
        # Cr√©er un nouveau mod√®le en cas d'erreur
        return QuestionAnalyzer()


def pretrain_analyzer(analyzer):
    """Pr√©-entra√Æne le r√©seau avec quelques exemples repr√©sentatifs"""
    # R√®gles sp√©cifiques pour l'embedding des mots cl√©s
    factual_keywords = [
        "quelle",
        "qui",
        "quand",
        "o√π",
        "combien",
        "pourquoi",
        "capitale",
        "population",
        "superficie",
        "pr√©sident",
        "nombre",
        "habitant",
        "est",
    ]
    opinion_keywords = [
        "penses",
        "crois",
        "avis",
        "opinion",
        "pr√©f√®res",
        "meilleur",
        "pire",
    ]
    howto_keywords = [
        "comment",
        "proc√©dure",
        "√©tapes",
        "faire",
        "cr√©er",
        "d√©velopper",
        "fabriquer",
    ]

    # Cr√©er un dictionnaire word_to_idx fictif pour les mots cl√©s importants
    word_to_idx = {}
    for i, word in enumerate(factual_keywords):
        word_to_idx[word] = i + 10  # Commencer √† 10

    for i, word in enumerate(opinion_keywords):
        word_to_idx[word] = i + 100  # Opinion words start at index 100

    for i, word in enumerate(howto_keywords):
        word_to_idx[word] = i + 200  # How-to words start at index 200

    # Initialiser les embeddings pour les mots cl√©s
    for word, idx in word_to_idx.items():
        if idx < 100:  # Factual words
            # Biais vers la cat√©gorie factuelle (indice 0)
            analyzer.embedding.weight.data[idx] = torch.tensor(
                [
                    0.5 if j == 20 else 0.1 * random.random()
                    for j in range(analyzer.embedding.weight.data[idx].shape[0])
                ],
                dtype=torch.float,
            )
        elif idx < 200:  # Opinion words
            # Biais vers la cat√©gorie opinion (indice 1)
            analyzer.embedding.weight.data[idx] = torch.tensor(
                [
                    0.5 if j == 10 else 0.1 * random.random()
                    for j in range(analyzer.embedding.weight.data[idx].shape[0])
                ],
                dtype=torch.float,
            )
        else:  # How-to words
            # Biais vers la cat√©gorie how-to (indice 2)
            analyzer.embedding.weight.data[idx] = torch.tensor(
                [
                    0.5 if j == 30 else 0.1 * random.random()
                    for j in range(analyzer.embedding.weight.data[idx].shape[0])
                ],
                dtype=torch.float,
            )


def generate_response(
    prompt,
    model,
    tokenizer,
    analyzer,
    max_length=150,
    temperature=0.8,
    top_p=0.9,
    repetition_penalty=1.2,
):
    """G√©n√®re une r√©ponse bas√©e sur le prompt donn√©, en utilisant l'analyseur de questions"""
    # D'abord essayer de trouver une r√©ponse dans les r√©ponses pr√©d√©finies
    factual_answer = get_factual_answer(prompt)
    if factual_answer:
        return factual_answer, "factual"

    # Si pas de correspondance, analyser la question pour d√©terminer son type
    question_type, confidence = analyzer.predict(prompt, tokenizer)

    # Construire un prompt appropri√© selon le type de question
    if question_type == "factual":
        # Pour les questions factuelles, utiliser un prompt tr√®s directif
        temperature = 0.3  # Temp√©rature tr√®s basse pour des r√©ponses plus d√©terministes
        max_length = 50  # Limiter la longueur pour √©viter les divagations

        # Construire un prompt sp√©cifique aux faits
        context = "R√©ponds √† cette question factuelle de mani√®re concise et directe. "
        prefix = "Question: "
        suffix = "\nR√©ponse factuelle: "

        # Pour certains types sp√©cifiques de questions, donner des indices
        if "capitale" in prompt.lower():
            context += (
                "Les capitales des pays sont des informations factuelles pr√©cises. "
            )
        elif "population" in prompt.lower() or "habitant" in prompt.lower():
            context += (
                "Les donn√©es d√©mographiques sont des informations factuelles pr√©cises. "
            )

    elif question_type == "opinion":
        # Pour les questions d'opinion, permettre plus de cr√©ativit√©
        temperature = 0.8
        context = (
            "Cette question demande ton avis. Tu peux exprimer une opinion √©quilibr√©e. "
        )
        prefix = "Question d'opinion: "
        suffix = "\nMon point de vue: "

    elif question_type == "how-to":
        # Pour les questions de proc√©dure
        temperature = 0.7
        context = (
            "Cette question demande une proc√©dure. Explique les √©tapes clairement. "
        )
        prefix = "Comment: "
        suffix = "\nVoici les √©tapes: "

    else:
        # Par d√©faut
        context = "R√©ponds √† cette question de mani√®re claire et concise. "
        prefix = "Question: "
        suffix = "\nR√©ponse: "

    # Construire le prompt final
    full_prompt = f"{context}{prefix}{prompt.strip()}{suffix}"

    # Tokenize et convertir en tensor
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    # Configurer les param√®tres de g√©n√©ration sp√©cifiques au type de question
    gen_kwargs = {
        "max_length": max_length + len(inputs["input_ids"][0]),
        "temperature": temperature,
        "top_p": 0.85 if question_type == "factual" else top_p,
        "repetition_penalty": 1.5 if question_type == "factual" else repetition_penalty,
        "do_sample": not question_type
        == "factual",  # Pour les questions factuelles, d√©sactiver le sampling
        "top_k": 10 if question_type == "factual" else 30,
        "no_repeat_ngram_size": 3,
        "num_beams": 5 if question_type == "factual" else 3,
        "early_stopping": True,
        "length_penalty": 0.5 if question_type == "factual" else 1.0,
    }

    # G√©n√©rer la r√©ponse
    set_seed(random.randint(1, 1000))  # Pour la reproductibilit√© mais avec variation
    output_sequences = model.generate(**inputs, **gen_kwargs)

    # D√©coder et nettoyer la r√©ponse
    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)

    try:
        # Extraire uniquement la partie r√©ponse en utilisant le s√©parateur
        response = generated_text.split(suffix)[-1].strip()
    except:
        # En cas d'√©chec du split, utiliser tout le texte g√©n√©r√©
        response = generated_text.strip()

    # Nettoyer la r√©ponse
    response = clean_response(response)

    # Post-traitement sp√©cifique pour les questions factuelles
    if question_type == "factual":
        # Pour les questions factuelles, prendre seulement la premi√®re phrase
        response = extract_first_sentence(response)

    return response, question_type


def extract_first_sentence(text):
    """Extrait la premi√®re phrase compl√®te d'un texte"""
    # Chercher la fin de la premi√®re phrase
    sentence_end = re.search(r"[.!?](\s|$)", text)
    if sentence_end:
        end_pos = sentence_end.end()
        return text[:end_pos].strip()

    # Si pas de ponctuation de fin de phrase trouv√©e, retourner le texte entier
    return text.strip()


def clean_response(response):
    """Nettoie la r√©ponse des artefacts courants et am√©liore sa lisibilit√©"""
    # √âliminer les phrases qui contiennent des mots cl√©s probl√©matiques
    problem_keywords = [
        "Naissances",
        "Portail",
        "Cat√©gorie:",
        "Le Monde",
        "Notes",
        "R√©f√©rences",
        "Articles connexes",
    ]

    # Diviser en phrases
    sentences = re.split(r"([.!?]\s+)", response)
    cleaned_sentences = []
    current_sentence = ""

    for i in range(0, len(sentences)):
        if i % 2 == 0:  # Partie de texte
            current_sentence = sentences[i]
        else:  # Ponctuation
            current_sentence += sentences[i]

            # V√©rifier si la phrase contient des mots probl√©matiques
            if not any(keyword in current_sentence for keyword in problem_keywords):
                cleaned_sentences.append(current_sentence)

    cleaned_response = "".join(cleaned_sentences)

    # Nettoyer les r√©f√©rences Wikipedia, hashtags, etc.
    cleaned_response = re.sub(r"#[a-zA-Z0-9_]+", "", cleaned_response)
    cleaned_response = re.sub(r"\[\d+\]", "", cleaned_response)
    cleaned_response = re.sub(r"\(lire en ligne\)", "", cleaned_response)

    # Enlever les lignes qui commencent par des caract√®res sp√©ciaux ou des statistiques
    cleaned_response = re.sub(
        r"^\s*[#\*\-][^\n]*$", "", cleaned_response, flags=re.MULTILINE
    )

    # √âliminer les lignes trop courtes (souvent des artefacts)
    cleaned_response = re.sub(r"^\s*.{1,10}$", "", cleaned_response, flags=re.MULTILINE)

    # √âliminer les sauts de ligne multiples et espaces en trop
    cleaned_response = re.sub(r"\n+", "\n", cleaned_response)
    cleaned_response = re.sub(r" +", " ", cleaned_response)

    # Si apr√®s nettoyage la r√©ponse est trop courte, renvoyez un message d'excuse
    if len(cleaned_response.strip()) < 10:
        return "Je ne peux pas fournir une r√©ponse pr√©cise √† cette question avec les informations dont je dispose."

    # Ajout√©: v√©rifier si la r√©ponse est une question qui r√©p√®te la question originale
    if re.match(r"^(qu|qui|que|quoi|comment|pourquoi|quand|o√π)", response.lower()):
        if (
            "?" in response[:50]
        ):  # Si c'est une question dans les 50 premiers caract√®res
            return "D√©sol√©, je ne dispose pas de suffisamment d'informations pr√©cises pour r√©pondre √† cette question."

    return cleaned_response.strip()


def main():
    """Point d'entr√©e principal de l'assistant"""
    # Charger le mod√®le et le tokenizer
    model, tokenizer = load_model()

    # Charger le r√©seau d'analyse de questions
    analyzer = create_or_load_analyzer()

    # Afficher l'en-t√™te
    print("\nüí¨ Assistant IA Fran√ßais avec r√©seau neuronal d'analyse")
    print("=" * 50)
    print("üìå COMMANDES:")
    print(" - Tapez votre question et appuyez sur Entr√©e")
    print(" - Tapez 'q' pour quitter")
    print("=" * 50)

    # Boucle d'interaction
    while True:
        print("\n‚û§ ", end="")
        user_input = input().strip()

        if user_input.lower() in ["q", "quit", "exit"]:
            break

        if not user_input:
            continue

        print("\nüß† R√©flexion en cours...")
        start_time = time.time()

        try:
            # Faire jusqu'√† 3 tentatives pour obtenir une r√©ponse satisfaisante
            max_attempts = 3
            response = None
            question_type = None
            
            # Pour la premi√®re tentative, d√©finir une longueur standard
            initial_max_length = 150
            
            for attempt in range(max_attempts):
                # G√©n√©rer la r√©ponse avec l'aide du r√©seau d'analyse
                # Utiliser la longueur appropri√©e selon le type de question (uniquement si d√©j√† d√©termin√©)
                current_max_length = 80 if question_type == "factual" else initial_max_length
                
                response, question_type = generate_response(
                    user_input,
                    model,
                    tokenizer,
                    analyzer,
                    max_length=current_max_length,
                    temperature=0.7 - (attempt * 0.2),  # R√©duction de temp√©rature √† chaque tentative
                    repetition_penalty=1.2 + (attempt * 0.3)  # Augmentation de p√©nalit√© √† chaque tentative
                )

                # V√©rifier si la r√©ponse est acceptable
                if is_response_acceptable(response, question_type):
                    break  # R√©ponse acceptable

                # Si ce n'est pas la derni√®re tentative, informer l'utilisateur
                if attempt < max_attempts - 1:
                    print(f"(Am√©lioration de la r√©ponse, tentative {attempt+2}/{max_attempts}...)")

            # Si apr√®s toutes les tentatives, la r√©ponse est toujours mauvaise pour une question factuelle
            if question_type == "factual" and not is_response_acceptable(response, question_type):
                response = "Je ne dispose pas d'informations pr√©cises sur ce sujet. Pour les questions factuelles, je peux r√©pondre avec certitude sur des sujets comme les capitales des pays, populations approximatives, ou faits historiques importants."

            elapsed_time = time.time() - start_time

            print(f"\nü§ñ R√©ponse (type: {question_type}, temps: {elapsed_time:.2f}s):")
            print(response)
            print("\n" + "=" * 50)

        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
            import traceback

            traceback.print_exc()

    print("\nAu revoir! üëã")

    # Sauvegarder les am√©liorations du r√©seau d'analyse (en production)
    # torch.save(analyzer.state_dict(), os.path.join(os.path.dirname(os.path.abspath(__file__)), "question_analyzer.pt"))


def is_response_acceptable(response, question_type):
    """V√©rifie si une r√©ponse est acceptable selon son type"""
    # Pour les questions factuelles, crit√®res plus stricts
    if question_type == "factual":
        # La r√©ponse ne doit pas √™tre une question
        if "?" in response[:50]:
            return False
        # Doit contenir des verbes d'affirmation typiques
        if not re.search(
            r"\b(est|sont|a|ont|√©tait|√©taient|fait|se trouve|se situe|compte|contient)\b",
            response,
            re.IGNORECASE,
        ):
            return False
        # Longueur minimale et maximale
        if len(response) < 10 or len(response) > 150:
            return False

    # Crit√®res g√©n√©raux pour tous les types de questions
    # Pas de mots-cl√©s probl√©matiques
    if any(word in response for word in ["Naissances", "Portail", "Cat√©gorie"]):
        return False
    # Pas de r√©ponse qui finit ou commence par une question
    if response.endswith("?") or response.startswith("?"):
        return False
    # Longueur minimale
    if len(response) < 10:
        return False

    return True


if __name__ == "__main__":
    main()
