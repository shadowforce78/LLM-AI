from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re

# Configuration
MODEL_PATH = "models/trained"  # Chemin principal du mod√®le entra√Æn√©
FALLBACK_PATH = "trained_llm"  # Chemin alternatif

def init_model_and_tokenizer():
    """Initialise le mod√®le et le tokenizer"""
    print("‚è≥ Chargement du mod√®le et du tokenizer...")
    
    try:
        # V√©rifier le chemin principal d'abord
        import os
        model_path = MODEL_PATH if os.path.exists(MODEL_PATH) else FALLBACK_PATH
        print(f"Utilisation du mod√®le depuis: {model_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # Configuration des tokens sp√©ciaux
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        
        print(f"‚úÖ Mod√®le charg√© sur {device.upper()}")
        return model, tokenizer, device
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
        return None, None, "cpu"

def format_prompt(text):
    """Formatte le prompt pour une meilleure g√©n√©ration"""
    return f"### Question : {text}\n\n### R√©ponse : "

def generate_model_response(model, tokenizer, device, question, max_length=150):
    """G√©n√®re une r√©ponse bas√©e uniquement sur le mod√®le entra√Æn√©"""
    # Formatage du prompt
    prompt = format_prompt(question)
    
    # Tokenisation
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        add_special_tokens=True
    ).to(device)
    
    # G√©n√©ration
    try:
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=inputs.input_ids.size(1) + max_length,
                do_sample=True,
                temperature=0.8,
                top_k=40,
                top_p=0.9,
                repetition_penalty=1.1,
                no_repeat_ngram_size=3,
                num_beams=1,  # Simple greedy decoding pour la vitesse
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # D√©codage de la sortie
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraction de la r√©ponse
        if "### R√©ponse :" in generated_text:
            response = generated_text.split("### R√©ponse :")[1].strip()
        else:
            response = generated_text.split(question)[1].strip()
        
        # Nettoyage
        response = clean_response(response)
        
        return response
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration: {str(e)}")
        return "Je ne peux pas r√©pondre √† cette question pour le moment."

def clean_response(text):
    """Nettoie la r√©ponse g√©n√©r√©e"""
    # Suppression des artefacts communs
    patterns_to_remove = [
        r'###.*?$',
        r'Notes et r√©f√©rences.*$',
        r'Liens externes.*$',
        r'Voir aussi.*$',
        r'Bibliographie.*$',
        r'Articles connexes.*$',
        r'^\s*\[\d+\]'
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.DOTALL|re.MULTILINE)
    
    # Normaliser les espaces et nettoyer
    text = re.sub(r'\s+', ' ', text).strip()
    
    # V√©rification de la qualit√© minimale
    if len(text) < 5 or text.count(' ') < 1:
        return "Je n'ai pas de r√©ponse pr√©cise √† cette question."
        
    return text

# Interface utilisateur
def main():
    # Initialisation
    model, tokenizer, device = init_model_and_tokenizer()
    
    print("\nüí¨ Assistant IA Fran√ßais")
    print("=" * 50)
    print("üìå COMMANDES:")
    print(" - Tapez votre question et appuyez sur Entr√©e")
    print(" - Tapez 'q' pour quitter")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\n‚û§ ").strip()
            
            if not user_input:
                continue
                
            if user_input.lower() in ['q', 'quit', 'exit']:
                print("\nAu revoir! üëã")
                break
            
            # Traitement avec le mod√®le
            if model is not None:
                print("\nüß† R√©flexion en cours...")
                response = generate_model_response(model, tokenizer, device, user_input)
                print("\nü§ñ R√©ponse (g√©n√©r√©e par le mod√®le):")
                print(response)
            else:
                print("\n‚ùå Mod√®le non disponible.")
                
            print("\n" + "=" * 50)
            
        except KeyboardInterrupt:
            print("\nOp√©ration annul√©e par l'utilisateur.")
            print("\nAu revoir! üëã")
            break
        except Exception as e:
            print(f"\n‚ùå Erreur: {str(e)}")
            print("Veuillez r√©essayer.")

if __name__ == "__main__":
    main()
