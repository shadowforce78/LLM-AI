from tokenizers import Tokenizer, models, pre_tokenizers, trainers
import os
import json

# üîç D√©terminer le chemin racine du projet
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# üìÇ Dossier contenant les textes nettoy√©s (chemins relatifs √† la racine du projet)
DATA_DIR = os.path.join(PROJECT_ROOT, "data", "cleaned")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "tokenized")
VOCAB_SIZE = 32_000  # Taille du vocabulaire √† d√©finir

# üìå Charger les textes
print(f"üìÇ Chargement des textes depuis {DATA_DIR} et ses sous-dossiers...")
corpus = []
file_count = 0

# Fonction pour parcourir les dossiers de mani√®re r√©cursive
def load_files_from_dir(directory):
    global corpus, file_count
    
    for item_name in os.listdir(directory):
        item_path = os.path.join(directory, item_name)
        
        # Si c'est un dossier, on le parcourt r√©cursivement
        if os.path.isdir(item_path):
            print(f"üìÅ Exploration du sous-dossier: {item_name}")
            load_files_from_dir(item_path)
        
        # Si c'est un fichier JSON, on le traite
        elif os.path.isfile(item_path) and item_name.endswith('.json'):
            try:
                with open(item_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for article in data:
                        corpus.append(article["text"])
                file_count += 1
                print(f"‚úì Fichier {item_name} dans {os.path.basename(directory)} charg√© avec succ√®s")
            except PermissionError:
                print(f"‚ö†Ô∏è Erreur de permission: Impossible de lire {item_path}")
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è Erreur: {item_path} n'est pas un fichier JSON valide")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du chargement de {item_path}: {str(e)}")

# Lancer le chargement des fichiers
load_files_from_dir(DATA_DIR)

if not corpus:
    print("‚ö†Ô∏è Attention: Aucun texte n'a √©t√© charg√©. V√©rifiez le contenu du dossier.")
    exit(1)
else:
    print(f"‚úÖ {len(corpus)} articles charg√©s depuis {file_count} fichiers avec succ√®s")

# üß© D√©finition du tokenizer WordPiece
tokenizer = Tokenizer(models.BPE())  # Peut √™tre chang√© en Unigram ou WordPiece
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# üìö Entra√Ænement du tokenizer
trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"])
tokenizer.train_from_iterator(corpus, trainer)

# üìÇ Sauvegarde du tokenizer
os.makedirs(OUTPUT_DIR, exist_ok=True)
tokenizer.save(os.path.join(OUTPUT_DIR, "tokenizer.json"))

print(f"‚úÖ Tokenizer entra√Æn√© et sauvegard√© dans {OUTPUT_DIR}/tokenizer.json")
