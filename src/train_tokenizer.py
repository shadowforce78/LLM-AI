from tokenizers import Tokenizer, models, pre_tokenizers, trainers
import os
import json

# ğŸ” DÃ©terminer le chemin racine du projet
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# ğŸ“‚ Dossier contenant les textes nettoyÃ©s (chemins relatifs Ã  la racine du projet)
DATA_DIR = os.path.join(PROJECT_ROOT, "data", "cleaned")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "tokenized")
VOCAB_SIZE = 32_000  # Taille du vocabulaire Ã  dÃ©finir

# ğŸ“Œ Charger les textes
corpus = []
for file in os.listdir(DATA_DIR):
    with open(os.path.join(DATA_DIR, file), "r", encoding="utf-8") as f:
        data = json.load(f)
        for article in data:
            corpus.append(article["text"])

# ğŸ§© DÃ©finition du tokenizer WordPiece
tokenizer = Tokenizer(models.BPE())  # Peut Ãªtre changÃ© en Unigram ou WordPiece
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# ğŸ“š EntraÃ®nement du tokenizer
trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"])
tokenizer.train_from_iterator(corpus, trainer)

# ğŸ“‚ Sauvegarde du tokenizer
os.makedirs(OUTPUT_DIR, exist_ok=True)
tokenizer.save(os.path.join(OUTPUT_DIR, "tokenizer.json"))

print(f"âœ… Tokenizer entraÃ®nÃ© et sauvegardÃ© dans {OUTPUT_DIR}/tokenizer.json")
