from tokenizers import Tokenizer, models, pre_tokenizers, trainers
import os
import json

# ğŸ” DÃ©terminer le chemin racine du projet
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# ğŸ“‚ Dossier contenant les textes nettoyÃ©s (chemins relatifs Ã  la racine du projet)
DATA_DIR = os.path.join(PROJECT_ROOT, "data", "cleaned")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "tokenized")
VOCAB_SIZE = 32_000  # Taille du vocabulaire Ã  dÃ©finir

# ğŸ“Œ Charger les textes
print(f"ğŸ“‚ Chargement des textes depuis {DATA_DIR} et ses sous-dossiers...")
corpus = []
file_count = 0

# Fonction pour parcourir les dossiers de maniÃ¨re rÃ©cursive
def load_files_from_dir(directory):
    global corpus, file_count
    
    for item_name in os.listdir(directory):
        item_path = os.path.join(directory, item_name)
        
        # Si c'est un dossier, on le parcourt rÃ©cursivement
        if os.path.isdir(item_path):
            print(f"ğŸ“ Exploration du sous-dossier: {item_name}")
            load_files_from_dir(item_path)
        
        # Si c'est un fichier JSON, on le traite
        elif os.path.isfile(item_path) and item_name.endswith('.json'):
            try:
                with open(item_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for article in data:
                        corpus.append(article["text"])
                file_count += 1
                print(f"âœ“ Fichier {item_name} dans {os.path.basename(directory)} chargÃ© avec succÃ¨s")
            except PermissionError:
                print(f"âš ï¸ Erreur de permission: Impossible de lire {item_path}")
            except json.JSONDecodeError:
                print(f"âš ï¸ Erreur: {item_path} n'est pas un fichier JSON valide")
            except Exception as e:
                print(f"âš ï¸ Erreur lors du chargement de {item_path}: {str(e)}")

# Lancer le chargement des fichiers
load_files_from_dir(DATA_DIR)

if not corpus:
    print("âš ï¸ Attention: Aucun texte n'a Ã©tÃ© chargÃ©. VÃ©rifiez le contenu du dossier.")
    exit(1)
else:
    print(f"âœ… {len(corpus)} articles chargÃ©s depuis {file_count} fichiers avec succÃ¨s")

# ğŸ§© DÃ©finition du tokenizer - Utilisation du modÃ¨le BPE qui est plus flexible
tokenizer = Tokenizer(models.BPE(unk_token="[UNK]"))

# Configuration du pre-tokenizer pour un meilleur dÃ©coupage initial
# Passage de add_prefix_space Ã  False pour Ã©viter l'espace au dÃ©but lors du dÃ©codage
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

# DÃ©finition du dÃ©codeur correspondant au pre-tokenizer
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
tokenizer.decoder = ByteLevelDecoder()

# Normalisation amÃ©liorÃ©e pour assurer le lowercasing et la suppression des accents
from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents
tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])

# ğŸ“š EntraÃ®nement du tokenizer
trainer = trainers.BpeTrainer(
    vocab_size=VOCAB_SIZE, 
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]", "[BOS]", "[EOS]"],
    min_frequency=2,
    show_progress=True
)
print(f"ğŸ” EntraÃ®nement du tokenizer (modÃ¨le BPE) avec {len(corpus)} textes...")
tokenizer.train_from_iterator(corpus, trainer)

# Test rapide de validation
test_text = "Bonjour, je suis un texte d'exemple."
encoded = tokenizer.encode(test_text)
decoded = tokenizer.decode(encoded.ids)
print("\nğŸ§ª Test de validation:")
print(f"  Texte original: \"{test_text}\"")
print(f"  Tokens: {encoded.tokens}")
print(f"  Texte dÃ©codÃ©: \"{decoded}\"")
print("  Note: Le tokenizer convertit le texte en lowercase et peut modifier lÃ©gÃ¨rement la tokenisation.")

# VÃ©rification plus prÃ©cise tenant compte des transformations du tokenizer
lower_test = test_text.lower()
is_similar = lower_test in decoded or decoded in lower_test
print(f"  Correspondance approximative: {'âœ… OK' if is_similar else 'âŒ DiffÃ©rent'}")

# Test plus complet montrant l'utilisation correcte pour l'entrainement de modÃ¨les
print("\nğŸ”„ Test d'utilisation complÃ¨te:")
sample_texts = ["Bonjour le monde!", "Comment Ã§a va aujourd'hui?", "Le tokenizer est maintenant configurÃ©."]
for text in sample_texts:
    encoded = tokenizer.encode(text)
    decoded = tokenizer.decode(encoded.ids)
    print(f"  Original: \"{text}\"")
    print(f"  EncodÃ©: {encoded.ids[:10]}{'...' if len(encoded.ids) > 10 else ''}")
    print(f"  DÃ©codÃ©: \"{decoded}\"")
    print("  ---")

# ğŸ“‚ Sauvegarde du tokenizer
os.makedirs(OUTPUT_DIR, exist_ok=True)
tokenizer.save(os.path.join(OUTPUT_DIR, "tokenizer.json"))

print(f"âœ… Tokenizer entraÃ®nÃ© et sauvegardÃ© dans {OUTPUT_DIR}/tokenizer.json")
