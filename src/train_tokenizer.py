from tokenizers import Tokenizer, models, pre_tokenizers, trainers
import os
import json

# üîç D√©terminer le chemin racine du projet
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# üìÇ Dossier contenant les textes nettoy√©s (chemins relatifs √† la racine du projet)
DATA_DIR = os.path.join(PROJECT_ROOT, "data", "cleaned")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "tokenized")
VOCAB_SIZE = 32_000  # Taille du vocabulaire √† d√©finir

# üìå Charger les textes
print(f"üìÇ Chargement des textes depuis {DATA_DIR}...")
corpus = []
for file_name in os.listdir(DATA_DIR):
    file_path = os.path.join(DATA_DIR, file_name)
    # V√©rifier si c'est un fichier (et non un r√©pertoire) et qu'il a une extension .json
    if os.path.isfile(file_path) and file_name.endswith('.json'):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                for article in data:
                    corpus.append(article["text"])
            print(f"‚úì Fichier {file_name} charg√© avec succ√®s")
        except PermissionError:
            print(f"‚ö†Ô∏è Erreur de permission: Impossible de lire {file_name}")
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è Erreur: {file_name} n'est pas un fichier JSON valide")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du chargement de {file_name}: {str(e)}")

if not corpus:
    print("‚ö†Ô∏è Attention: Aucun texte n'a √©t√© charg√©. V√©rifiez le contenu du dossier.")
    exit(1)
else:
    print(f"‚úÖ {len(corpus)} articles charg√©s avec succ√®s")

# üß© D√©finition du tokenizer WordPiece
tokenizer = Tokenizer(models.BPE())  # Peut √™tre chang√© en Unigram ou WordPiece
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# üìö Entra√Ænement du tokenizer
trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"])
tokenizer.train_from_iterator(corpus, trainer)

# üìÇ Sauvegarde du tokenizer
os.makedirs(OUTPUT_DIR, exist_ok=True)
tokenizer.save(os.path.join(OUTPUT_DIR, "tokenizer.json"))

print(f"‚úÖ Tokenizer entra√Æn√© et sauvegard√© dans {OUTPUT_DIR}/tokenizer.json")
