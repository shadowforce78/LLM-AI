# LLM-AI French Language Model

Un modÃ¨le de langage en franÃ§ais basÃ© sur GPT-2, entraÃ®nÃ© sur des articles WikipÃ©dia.

## ğŸŒŸ CaractÃ©ristiques

- ModÃ¨le basÃ© sur GPT-2 franÃ§ais (dbddv01/gpt2-french-small)
- Fine-tuning sur des articles WikipÃ©dia en franÃ§ais
- Support des conversations et gÃ©nÃ©ration de texte
- Tokenizer optimisÃ© pour le franÃ§ais
- Gestion automatique des donnÃ©es d'entraÃ®nement

## ğŸš€ Installation

```bash
# Cloner le projet
git clone https://github.com/votre-username/LLM-AI.git
cd LLM-AI

# CrÃ©er un environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install transformers torch datasets wikipediaapi tqdm accelerate
```

## ğŸ“ Structure du Projet

```
LLM-AI/
â”œâ”€â”€ wiki-scrap.py        # Extraction des donnÃ©es Wikipedia
â”œâ”€â”€ tokenizer.py         # Configuration du tokenizer
â”œâ”€â”€ modele_base.py       # DÃ©finition du modÃ¨le
â”œâ”€â”€ train.py            # Script d'entraÃ®nement
â”œâ”€â”€ test.py             # Script de test
â””â”€â”€ trained_llm/        # Dossier du modÃ¨le entraÃ®nÃ©
```

## ğŸ”§ Utilisation

1. **Extraction des donnÃ©es** :
```bash
python wiki-scrap.py
```

2. **PrÃ©paration des tokens** :
```bash
python tokenizer.py
```

3. **EntraÃ®nement du modÃ¨le** :
```bash
python train.py
```

4. **Test du modÃ¨le** :
```bash
python test.py
```

## ğŸ“Š Exemple d'utilisation

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Charger le modÃ¨le et le tokenizer
tokenizer = AutoTokenizer.from_pretrained("trained_llm")
model = AutoModelForCausalLM.from_pretrained("trained_llm")

# GÃ©nÃ©rer du texte
prompt = "Quelle est la capitale de la France ?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
response = tokenizer.decode(outputs[0])
print(response)
```

## ğŸ¯ ParamÃ¨tres d'entraÃ®nement

- Taille du modÃ¨le : 124M paramÃ¨tres
- Epochs : 10
- Batch size : 4
- Learning rate : 5e-5
- Warmup steps : 500
- Beam search : 5 beams

## ğŸ“ Notes

- Le modÃ¨le utilise GPT-2 comme architecture de base
- Les donnÃ©es d'entraÃ®nement sont extraites de WikipÃ©dia
- Le tokenizer est optimisÃ© pour le franÃ§ais
- Le modÃ¨le supporte les tokens spÃ©ciaux (BOS, EOS, PAD)

## ğŸ“š DonnÃ©es

Les catÃ©gories d'articles WikipÃ©dia utilisÃ©es :
- Base : Histoire, gÃ©ographie et culture franÃ§aise
- Tech : IA, apprentissage automatique, deep learning

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit vos changements (`git commit -m 'Ajout d'une fonctionnalitÃ©'`)
4. Push sur la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## âš ï¸ Limitations

- Le modÃ¨le est entraÃ®nÃ© sur un nombre limitÃ© d'articles
- Les performances peuvent varier selon la complexitÃ© des requÃªtes
- L'utilisation de CPU peut ralentir significativement l'infÃ©rence