# LLM-AI French Language Model

Un modÃ¨le de langage en franÃ§ais basÃ© sur GPT-2, entraÃ®nÃ© sur des articles WikipÃ©dia.

## ğŸŒŸ CaractÃ©ristiques

- ModÃ¨le basÃ© sur GPT-2 franÃ§ais (dbddv01/gpt2-french-small)
- Fine-tuning sur des articles WikipÃ©dia en franÃ§ais
- Support des conversations et gÃ©nÃ©ration de texte
- Tokenizer optimisÃ© pour le franÃ§ais
- Gestion automatique des donnÃ©es d'entraÃ®nement

## ğŸš€ Installation

```bash
# Cloner le projet
git clone https://github.com/votre-username/LLM-AI.git
cd LLM-AI

# CrÃ©er un environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install transformers torch datasets wikipediaapi tqdm accelerate
```

## ğŸ“ Structure du Projet

```
LLM-AI/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # DonnÃ©es brutes extraites de WikipÃ©dia
â”‚   â”œâ”€â”€ processed/        # DonnÃ©es traitÃ©es prÃªtes pour l'entraÃ®nement
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ wiki-scrap.py     # Extraction des donnÃ©es Wikipedia
â”‚   â”œâ”€â”€ tokenizer.py      # Configuration du tokenizer
â”‚   â”œâ”€â”€ train.py          # Script d'entraÃ®nement
â”‚   â”œâ”€â”€ test.py           # Script de test
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base_model/       # ModÃ¨le de base non entraÃ®nÃ©
â”‚   â”œâ”€â”€ trained_model/    # ModÃ¨le entraÃ®nÃ©
â””â”€â”€ notebooks/            # Notebooks Jupyter pour l'exploration et l'analyse
```

## ğŸ”§ Utilisation

1. **Extraction des donnÃ©es** :

```bash
python scripts/wiki-scrap.py
```

2. **PrÃ©paration des tokens** :

```bash
python scripts/tokenizer.py
```

3. **EntraÃ®nement du modÃ¨le** :

```bash
python scripts/train.py
```

4. **Test du modÃ¨le** :

```bash
python scripts/test.py
```

## ğŸ“Š Exemple d'utilisation

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Charger le modÃ¨le et le tokenizer
tokenizer = AutoTokenizer.from_pretrained("models/trained_model")
model = AutoModelForCausalLM.from_pretrained("models/trained_model")

# GÃ©nÃ©rer du texte
prompt = "Quelle est la capitale de la France ?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
response = tokenizer.decode(outputs[0])
print(response)
```

## ğŸ¯ ParamÃ¨tres d'entraÃ®nement

- Taille du modÃ¨le : 124M paramÃ¨tres
- Epochs : 10
- Batch size : 4
- Learning rate : 5e-5
- Warmup steps : 500
- Beam search : 5 beams

## ğŸ“ Notes

- Le modÃ¨le utilise GPT-2 comme architecture de base
- Les donnÃ©es d'entraÃ®nement sont extraites de WikipÃ©dia
- Le tokenizer est optimisÃ© pour le franÃ§ais
- Le modÃ¨le supporte les tokens spÃ©ciaux (BOS, EOS, PAD)

## ğŸ“š DonnÃ©es

Les catÃ©gories d'articles WikipÃ©dia utilisÃ©es :

- Base : Histoire, gÃ©ographie et culture franÃ§aise
- Tech : IA, apprentissage automatique, deep learning

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit vos changements (`git commit -m 'Ajout d'une fonctionnalitÃ©'`)
4. Push sur la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## âš ï¸ Limitations

- Le modÃ¨le est entraÃ®nÃ© sur un nombre limitÃ© d'articles
- Les performances peuvent varier selon la complexitÃ© des requÃªtes
- L'utilisation de CPU peut ralentir significativement l'infÃ©rence
