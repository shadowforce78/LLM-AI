# LLM-AI French Language Model

Un modÃ¨le de langage en franÃ§ais basÃ© sur GPT-2, entraÃ®nÃ© sur un vaste corpus d'articles WikipÃ©dia pour produire des rÃ©ponses pertinentes et informatives.

## ğŸŒŸ CaractÃ©ristiques

- ModÃ¨le basÃ© sur GPT-2 franÃ§ais (dbddv01/gpt2-french-small)
- EntraÃ®nement avancÃ© sur un large corpus d'articles WikipÃ©dia en franÃ§ais
- Traitement multiprocessing optimisÃ© pour l'extraction de donnÃ©es
- Assistant IA simple qui s'appuie uniquement sur ses connaissances apprises
- Exploration approfondie de WikipÃ©dia avec 6 catÃ©gories thÃ©matiques
- Support multi-plateforme (Windows, Linux, macOS)

## ğŸš€ Installation

```bash
# Cloner le projet
git clone https://github.com/votre-username/LLM-AI.git
cd LLM-AI

# CrÃ©er un environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install transformers torch datasets wikipediaapi tqdm accelerate tensorboard
```

## ğŸ“ Structure du Projet

```
LLM-AI/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # DonnÃ©es brutes extraites de WikipÃ©dia
â”‚   â””â”€â”€ tokenized_dataset/ # DonnÃ©es tokenisÃ©es prÃªtes pour l'entraÃ®nement
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ wiki-scrap.py     # Extraction massive de donnÃ©es Wikipedia
â”‚   â”œâ”€â”€ tokenizer.py      # Configuration et entraÃ®nement du tokenizer
â”‚   â”œâ”€â”€ train.py          # Script d'entraÃ®nement optimisÃ©
â”‚   â””â”€â”€ ai_assistant.py   # Interface utilisateur pour interagir avec le modÃ¨le
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/             # Configuration du modÃ¨le de base
â”‚   â””â”€â”€ trained/          # ModÃ¨le entraÃ®nÃ© (rÃ©sultat final)
â”œâ”€â”€ scripts/              # Scripts utilitaires pour faciliter l'utilisation
â””â”€â”€ docs/                 # Documentation du projet
```

## ğŸ”§ Utilisation

Le projet propose une approche complÃ¨te pour crÃ©er un modÃ¨le LLM personnalisÃ© en franÃ§ais:

### 1. Extraction de donnÃ©es (Scraping)

Le script collecte automatiquement un vaste corpus d'articles WikipÃ©dia Ã  travers 6 catÃ©gories principales:
- Base (France, villes, gÃ©ographie...)
- Tech (IA, informatique, rÃ©seaux...)
- Sciences (physique, biologie, mathÃ©matiques...)
- Culture (arts, musique, littÃ©rature...)
- GÃ©ographie (continents, pays, formations gÃ©ographiques...)
- Histoire (pÃ©riodes, Ã©vÃ©nements, personnalitÃ©s...)

```bash
python src/wiki-scrap.py
```

### 2. PrÃ©paration des donnÃ©es

Tokenisation des donnÃ©es extraites pour les rendre exploitables par le modÃ¨le:

```bash
python src/tokenizer.py
```

## ğŸ”¤ Le Tokenizer

Le tokenizer est un composant essentiel du pipeline de traitement du langage naturel. Il transforme le texte brut en tokens (unitÃ©s lexicales) que le modÃ¨le peut comprendre.

### CaractÃ©ristiques du tokenizer

- **Base**: AdaptÃ© de GPT-2 franÃ§ais (dbddv01/gpt2-french-small)
- **Vocabulaire**: ~50K tokens optimisÃ©s pour le franÃ§ais
- **Tokens spÃ©ciaux**:
  - `<bos>`: Marqueur de dÃ©but de sÃ©quence
  - `<eos>`: Marqueur de fin de sÃ©quence
  - `<pad>`: Token de padding pour uniformiser les longueurs
  - `<unk>`: Token pour les mots inconnus
- **StratÃ©gie de padding**: CÃ´tÃ© gauche (`padding_side="left"`)
- **StratÃ©gie de troncature**: CÃ´tÃ© gauche (`truncation_side="left"`)

### Traitement des donnÃ©es

Le tokenizer applique aux donnÃ©es brutes:
1. **Normalisation**: Standardisation du texte
2. **Segmentation**: Division en tokens pertinents
3. **Transformation Q&A**: Formatage des donnÃ©es en paires question-rÃ©ponse
4. **Padding/Truncation**: Uniformisation des sÃ©quences Ã  une longueur de 512 tokens

### Exemple de tokenisation

```python
# Exemple de tokenisation d'une question
question = "Quelle est la capitale de la France ?"
tokens = tokenizer.encode(question)
# => [1, 1158, 318, 287, 2255, 293, 287, 1567, 30, 13]

# DÃ©tokenisation
tokenizer.decode(tokens)
# => "<bos>Quelle est la capitale de la France ?<eos>"
```

Cette approche permet au modÃ¨le de traiter efficacement le franÃ§ais avec ses particularitÃ©s linguistiques.

### 3. EntraÃ®nement du modÃ¨le

Processus d'entraÃ®nement optimisÃ© avec suivi des mÃ©triques et sauvegarde des meilleurs checkpoints:

```bash
python src/train.py
```

### 4. Utilisation du modÃ¨le

L'assistant IA peut Ãªtre utilisÃ© via l'interface en ligne de commande:

```bash
python src/ai_assistant.py
# ou
scripts/use.bat  # Sur Windows
```

## ğŸ“Š FonctionnalitÃ©s avancÃ©es

### Extraction parallÃ©lisÃ©e des donnÃ©es

- **Multiprocessing** sur Linux/Unix et **Multithreading** sur Windows
- Exploration multi-niveaux des articles liÃ©s
- Ã‰chantillonnage alÃ©atoire pour diversifier les sources
- Sauvegardes intermÃ©diaires pour Ã©viter la perte de donnÃ©es

### EntraÃ®nement optimisÃ©

- Ajustement automatique des hyperparamÃ¨tres selon la taille du dataset
- Monitoring avec TensorBoard
- DÃ©tection et sauvegarde des meilleurs modÃ¨les
- Early stopping intelligent avec pÃ©riode minimale garantie
- Visualisation ASCII de l'Ã©volution de la perte en temps rÃ©el

### Assistant IA minimaliste

- Interface simple en ligne de commande
- GÃ©nÃ©ration de rÃ©ponses basÃ©es uniquement sur les connaissances apprises
- Support des questions factuelles, opinions et instructions

## ğŸ“š Sources de donnÃ©es

L'assistant est entraÃ®nÃ© sur les catÃ©gories d'articles WikipÃ©dia suivantes:

- **Base**: France, Paris, Lyon, Marseille, Bordeaux, Toulouse, Strasbourg, et bien d'autres articles liÃ©s Ã  la gÃ©ographie franÃ§aise, l'histoire franÃ§aise et la culture franÃ§aise
- **Tech**: Intelligence artificielle, apprentissage automatique, deep learning, rÃ©seaux informatiques, cybersÃ©curitÃ©, blockchain, cloud computing...
- **Sciences**: Biologie, chimie, physique, mathÃ©matiques, astronomie, gÃ©nÃ©tique, neurosciences, mÃ©decine...
- **Culture**: Musique, cinÃ©ma, littÃ©rature, peinture, sculpture, danse, architecture, jeux vidÃ©o...
- **GÃ©ographie**: Continents, pays, montagnes, ocÃ©ans, mers, fleuves, climats, dÃ©serts...
- **Histoire**: AntiquitÃ©, Moyen Ã‚ge, Renaissance, guerres mondiales, civilisations, empires...

Au total, plus de 100 thÃ¨mes principaux sont explorÃ©s, chacun gÃ©nÃ©rant de nombreux articles liÃ©s.

## ğŸ” Performances

Le modÃ¨le est entraÃ®nÃ© pour minimiser la perte tout en Ã©vitant le surapprentissage, avec des caractÃ©ristiques:

- **Vocabulaire**: Tokenizer franÃ§ais spÃ©cialisÃ© avec ~50K tokens
- **Nombre de paramÃ¨tres**: 124M (base GPT-2 small)
- **Longueur maximale de sÃ©quence**: 512 tokens en entraÃ®nement, extensible jusqu'Ã  1024 en infÃ©rence
- **Format d'entrÃ©e**: Paires de question-rÃ©ponse formatÃ©es avec tokens spÃ©ciaux
- **CapacitÃ© de gÃ©nÃ©ration**: Textes cohÃ©rents et informatifs en franÃ§ais

## ğŸ¤ Contribution

Les contributions sont les bienvenues! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit vos changements (`git commit -m 'Ajout d'une fonctionnalitÃ©'`)
4. Push sur la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## âš ï¸ Limitations

- Le modÃ¨le est limitÃ© aux informations contenues dans son corpus d'entraÃ®nement
- L'assistant ne possÃ¨de pas de connaissances sur les Ã©vÃ©nements postÃ©rieurs Ã  son entraÃ®nement
- Comme tous les modÃ¨les de langage, il peut parfois gÃ©nÃ©rer des informations incorrectes
- Les performances dÃ©pendent significativement du matÃ©riel utilisÃ© (GPU recommandÃ©)
