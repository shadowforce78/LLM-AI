import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_model_and_tokenizer(model_path="trained_llm"):
    """Charge le mod√®le et le tokenizer"""
    print("‚è≥ Chargement du mod√®le et du tokenizer...")
    
    # Charger le tokenizer et le mod√®le depuis les fichiers sauvegard√©s
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()  # Mode d'√©valuation
    
    return model, tokenizer, device

def generate_response(model, tokenizer, device, question, max_length=150):
    """G√©n√®re une r√©ponse bas√©e uniquement sur le mod√®le entra√Æn√©"""
    # Formatage du prompt
    prompt = f"{tokenizer.bos_token}### Question : {question}\n\n### R√©ponse :"
    
    # Tokenisation
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        add_special_tokens=True
    ).to(device)
    
    # G√©n√©ration
    print("üß† R√©flexion en cours...")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=inputs.input_ids.size(1) + max_length,
            do_sample=True,
            temperature=0.8,  # Temp√©rature l√©g√®rement plus √©lev√©e pour plus de cr√©ativit√©
            top_k=40,
            top_p=0.9,
            repetition_penalty=1.1,
            num_beams=3,  # Beam search pour une meilleure coh√©rence
            early_stopping=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # D√©codage de la sortie
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extraction de la r√©ponse
    if "### R√©ponse :" in generated_text:
        response = generated_text.split("### R√©ponse :")[1].strip()
    else:
        response = generated_text.split(question)[1].strip()
    
    # Nettoyage de base
    response = clean_response(response)
    
    return response

def clean_response(text):
    """Nettoie la r√©ponse sans supprimer trop d'informations"""
    import re
    
    # Supprimer les marqueurs et autres artefacts
    text = re.sub(r'###.*?$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*[-‚Ä¢*]\s*', '', text, flags=re.MULTILINE)  # Listes √† puces
    
    # Supprimer les notes et r√©f√©rences typiques de Wikipedia
    patterns_to_remove = [
        r'Notes et r√©f√©rences.*$',
        r'Liens externes.*$',
        r'Voir aussi.*$',
        r'Bibliographie.*$',
        r'Articles connexes.*$',
        r'^\s*\[\d+\]',  # Citations num√©riques
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.DOTALL|re.MULTILINE)
    
    # Normaliser les espaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Interface utilisateur
if __name__ == "__main__":
    # Charger le mod√®le et le tokenizer
    model, tokenizer, device = load_model_and_tokenizer()
    print(f"‚úÖ Mod√®le charg√© sur {device.upper()}")
    
    print("\nüí¨ Assistant IA - G√©n√©ration Pure")
    print("=" * 50)
    print("üß† Cet assistant r√©pond uniquement en utilisant ses connaissances apprises")
    print("üí° Il n'utilise aucune base de connaissances pr√©d√©finie")
    print("‚ùì Tapez 'q' pour quitter")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\n‚û§ ").strip()
            
            if user_input.lower() in ['q', 'quit', 'exit']:
                print("\nAu revoir! üëã")
                break
                
            if not user_input:
                continue
            
            response = generate_response(model, tokenizer, device, user_input)
            print("\nü§ñ R√©ponse g√©n√©r√©e:")
            print(response)
            print("\n" + "=" * 50)
            
        except Exception as e:
            print(f"\n‚ùå Erreur: {str(e)}")
            print("Veuillez r√©essayer avec une autre question.")
