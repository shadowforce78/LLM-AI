from transformers import AutoTokenizer, AutoModelForCausalLM, Pipeline, pipeline
import torch
import re

# Charger le tokenizer et le mod√®le
tokenizer = AutoTokenizer.from_pretrained("trained_llm")
model = AutoModelForCausalLM.from_pretrained(
    "trained_llm",
    trust_remote_code=True,
    local_files_only=True
)
model.eval()
print("‚úÖ Mod√®le et tokenizer charg√©s")

def format_prompt(text):
    """Formatte le prompt pour une meilleure g√©n√©ration"""
    # Contexte plus structur√© pour guider la g√©n√©ration
    formatted = (
        f"{tokenizer.bos_token}### Question : {text}\n\n### R√©ponse : "
    )
    return formatted

# Base de connaissances enrichie
KNOWLEDGE_BASE = {
    "capitale": {
        "France": "Paris est la capitale de la France. C'est la plus grande ville du pays et son centre √©conomique et culturel.",
        "default": "Je ne connais pas la capitale de ce pays."
    },
    "population": {
        "France": "La France compte environ 68 millions d'habitants (2024).",
        "Paris": "Paris compte environ 2,2 millions d'habitants, et son aire urbaine environ 12 millions d'habitants.",
        "default": "Je ne connais pas la population exacte."
    },
    "d√©finition": {
        "IA": "L'Intelligence Artificielle (IA) est un domaine de l'informatique qui vise √† cr√©er des syst√®mes capables de simuler l'intelligence humaine.",
        "default": "Je ne peux pas fournir une d√©finition pr√©cise pour ce terme."
    }
}

def get_knowledge_base_answer(question):
    """Recherche une r√©ponse dans la base de connaissances"""
    question = question.lower()
    
    # R√®gles de correspondance
    if any(word in question for word in ["habitant", "population"]):
        if "france" in question:
            return KNOWLEDGE_BASE["population"]["France"]
        if "paris" in question:
            return KNOWLEDGE_BASE["population"]["Paris"]
            
    if "capitale" in question and "france" in question:
        return KNOWLEDGE_BASE["capitale"]["France"]
        
    if ("qu'est" in question or "c'est quoi" in question) and "ia" in question:
        return KNOWLEDGE_BASE["d√©finition"]["IA"]
        
    return None

# Option pour activer/d√©sactiver la base de connaissances
USE_KNOWLEDGE_BASE = False  # Mettre √† False pour utiliser uniquement le mod√®le

# Am√©liorer les param√®tres de g√©n√©ration
def generate_response(prompt_text, max_new_tokens=100):
    """G√©n√®re une r√©ponse avec des param√®tres optimis√©s"""
    # V√©rifier d'abord la base de connaissances si activ√©e
    if USE_KNOWLEDGE_BASE:
        kb_answer = get_knowledge_base_answer(prompt_text)
        if kb_answer:
            return kb_answer

    # Essayer avec un prompt direct sans formatage complexe
    raw_prompt = f"{prompt_text}"
    
    inputs = tokenizer(
        raw_prompt,
        return_tensors="pt",
        truncation=True,
        max_length=256,  # R√©duit pour √©viter les probl√®mes de contexte
    )
    
    try:
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                min_new_tokens=5,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.7,  # Plus de cr√©ativit√©
                top_k=50,        # Moins restrictif
                top_p=0.9,
                repetition_penalty=1.2,  # Moins restrictif
                no_repeat_ngram_size=3,
                num_beams=1,      # Greedy decoding pour plus de spontan√©it√©
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # D√©codage basique
        generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        generated_text = generated_text.strip()
        
        # V√©rifier si la r√©ponse a du sens
        if len(generated_text) < 10 or "Notes et r√©f√©rences" in generated_text:
            # Fallback √† la base de connaissances si disponible
            fallback = get_fallback_answer(prompt_text)
            if fallback:
                return fallback
        
        return generated_text
    
    except Exception as e:
        print(f"Erreur de g√©n√©ration: {str(e)}")
        # Fallback √† la base de connaissances
        fallback = get_fallback_answer(prompt_text)
        if fallback:
            return fallback
        return "Je ne peux pas r√©pondre √† cette question pour le moment."

def get_fallback_answer(question):
    """Fournit une r√©ponse de secours pour les questions courantes"""
    question = question.lower()
    
    # Questions courantes et leurs r√©ponses
    fallbacks = {
        "capitale france": "Paris est la capitale de la France.",
        "habitant france": "La France compte environ 68 millions d'habitants (2024).",
        "intelligence artificielle": "L'Intelligence Artificielle (IA) est un domaine de l'informatique qui vise √† cr√©er des syst√®mes capables de simuler l'intelligence humaine.",
    }
    
    # Recherche de correspondance approximative
    for key, answer in fallbacks.items():
        if all(word in question for word in key.split()):
            return answer
            
    return None

def clean_response(text, original_prompt):
    """Nettoie et formate la r√©ponse g√©n√©r√©e"""
    # Extraire la r√©ponse apr√®s le marqueur
    try:
        response = re.split(r'###\s*R√©ponse\s*:', text)[-1].strip()
    except IndexError:
        response = text.replace(original_prompt, "").strip()
    
    # Nettoyage avanc√©
    response = re.sub(r'\s+', ' ', response)  # Normaliser les espaces
    response = re.sub(r'^\W+|\W+$', '', response)  # Nettoyer d√©but/fin
    response = re.sub(r'Portail.*$', '', response)  # Supprimer les mentions "Portail"
    response = re.sub(r'###.*$', '', response)  # Supprimer les marqueurs restants
    response = re.sub(r'Articles connexes.*$', '', response)
    response = re.sub(r'Liens externes.*$', '', response)
    response = re.sub(r'ISBN.*$', '', response)
    response = re.sub(r'\([^)]*\)', '', response)  # Supprimer les parenth√®ses
    
    # V√©rifications de qualit√©
    if len(response) < 10 or response.count(' ') < 2:
        return "Je ne peux pas g√©n√©rer une r√©ponse coh√©rente √† cette question."
    
    # Capitaliser la premi√®re lettre
    response = response[0].upper() + response[1:] if response else response
    
    return response

# Ajouter des questions d'exemple pour aider le mod√®le
EXAMPLE_PROMPTS = {
    "Quelle est la capitale de la France ?": "Paris est la capitale de la France.",
    "Qu'est-ce que l'IA ?": "L'Intelligence Artificielle (IA) est un domaine de l'informatique...",
}

def init_model_with_examples():
    """Initialise le mod√®le avec quelques exemples"""
    for q, a in EXAMPLE_PROMPTS.items():
        formatted = format_prompt(q) + a + tokenizer.eos_token
        _ = tokenizer(formatted, return_tensors="pt")

# Initialiser le mod√®le avec les exemples
init_model_with_examples()

# Interface utilisateur am√©lior√©e
print("\nüí¨ Assistant IA Fran√ßais - bas√© sur GPT-2")
print("=" * 50)
print("üìå COMMANDES:")
print(" - Tapez votre question et appuyez sur Entr√©e")
print(" - Tapez 'kb' pour activer/d√©sactiver la base de connaissances")
print(" - Tapez 'q' pour quitter")
print("=" * 50)
print(f"üìö Base de connaissances: {'ACTIV√âE' if USE_KNOWLEDGE_BASE else 'D√âSACTIV√âE'}")
print("=" * 50)

while True:
    try:
        user_input = input("\n‚û§ ").strip()
        if user_input.lower() in ['q', 'quit', 'exit']:
            print("\nAu revoir ! üëã")
            break
        
        if user_input.lower() == 'kb':
            USE_KNOWLEDGE_BASE = not USE_KNOWLEDGE_BASE
            print(f"\nüìö Base de connaissances: {'ACTIV√âE' if USE_KNOWLEDGE_BASE else 'D√âSACTIV√âE'}")
            continue
            
        if not user_input:
            continue
            
        print("\nü§î G√©n√©ration de la r√©ponse...")
        response = generate_response(user_input)
        print("\nü§ñ R√©ponse :")
        print(response)
        print("\n" + "=" * 50)
        
    except Exception as e:
        print(f"\n‚ùå Erreur: {str(e)}")
        print("Veuillez r√©essayer avec une autre question.")
