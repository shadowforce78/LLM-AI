from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re

# Charger le tokenizer et le mod√®le
tokenizer = AutoTokenizer.from_pretrained("trained_llm")
model = AutoModelForCausalLM.from_pretrained(
    "trained_llm",
    trust_remote_code=True,
    local_files_only=True
)
model.eval()
print("‚úÖ Mod√®le et tokenizer charg√©s")

def format_prompt(text):
    """Formatte le prompt pour une meilleure g√©n√©ration"""
    # Contexte plus structur√© pour guider la g√©n√©ration
    formatted = (
        f"{tokenizer.bos_token}### Question : {text}\n\n### R√©ponse : "
    )
    return formatted

# Base de connaissances pour les questions courantes
KNOWLEDGE_BASE = {
    "capitale": {
        "France": "Paris est la capitale de la France. C'est la plus grande ville du pays et son centre √©conomique et culturel.",
        "default": "Je ne connais pas la capitale de ce pays."
    },
    "d√©finition": {
        "IA": "L'Intelligence Artificielle (IA) est un domaine de l'informatique qui vise √† cr√©er des syst√®mes capables de simuler l'intelligence humaine.",
        "default": "Je ne peux pas fournir une d√©finition pr√©cise pour ce terme."
    }
}

def get_knowledge_base_answer(question):
    """Recherche une r√©ponse dans la base de connaissances"""
    question = question.lower()
    if "capitale" in question and "france" in question:
        return KNOWLEDGE_BASE["capitale"]["France"]
    return None

def generate_response(prompt_text, max_new_tokens=100):
    """G√©n√®re une r√©ponse avec des param√®tres optimis√©s"""
    # V√©rifier d'abord la base de connaissances
    kb_answer = get_knowledge_base_answer(prompt_text)
    if kb_answer:
        return kb_answer

    formatted_prompt = format_prompt(prompt_text)
    
    inputs = tokenizer(
        formatted_prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        add_special_tokens=True,
        padding=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            min_new_tokens=20,
            num_return_sequences=1,
            do_sample=True,
            temperature=0.3,  # R√©duit davantage pour plus de coh√©rence
            top_k=10,        # R√©duit pour plus de pr√©cision
            top_p=0.85,
            repetition_penalty=2.0,
            length_penalty=1.5,
            no_repeat_ngram_size=4,
            num_beams=5,
            early_stopping=True,
            pad_token_id=tokenizer.pad_token_id,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            bad_words_ids=[[tokenizer.pad_token_id]],
        )
    
    # D√©codage et nettoyage
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return clean_response(generated_text, prompt_text)

def clean_response(text, original_prompt):
    """Nettoie et formate la r√©ponse g√©n√©r√©e"""
    # Extraire la r√©ponse apr√®s le marqueur
    try:
        response = re.split(r'###\s*R√©ponse\s*:', text)[-1].strip()
    except IndexError:
        response = text.replace(original_prompt, "").strip()
    
    # Nettoyage avanc√©
    response = re.sub(r'\s+', ' ', response)  # Normaliser les espaces
    response = re.sub(r'^\W+|\W+$', '', response)  # Nettoyer d√©but/fin
    response = re.sub(r'Portail.*$', '', response)  # Supprimer les mentions "Portail"
    response = re.sub(r'###.*$', '', response)  # Supprimer les marqueurs restants
    
    # V√©rifications de qualit√©
    if len(response) < 10 or response.count(' ') < 2:
        return "Je ne peux pas g√©n√©rer une r√©ponse coh√©rente √† cette question."
    
    # Capitaliser la premi√®re lettre
    response = response[0].upper() + response[1:] if response else response
    
    return response

# Ajouter des questions d'exemple pour aider le mod√®le
EXAMPLE_PROMPTS = {
    "Quelle est la capitale de la France ?": "Paris est la capitale de la France.",
    "Qu'est-ce que l'IA ?": "L'Intelligence Artificielle (IA) est un domaine de l'informatique...",
}

def init_model_with_examples():
    """Initialise le mod√®le avec quelques exemples"""
    for q, a in EXAMPLE_PROMPTS.items():
        formatted = format_prompt(q) + a + tokenizer.eos_token
        _ = tokenizer(formatted, return_tensors="pt")

# Initialiser le mod√®le avec les exemples
init_model_with_examples()

# Interface utilisateur am√©lior√©e
print("\nüí¨ Assistant IA - Posez vos questions (ou 'q' pour quitter)")
print("=" * 50)

while True:
    try:
        user_input = input("\n‚û§ ").strip()
        if user_input.lower() in ['q', 'quit', 'exit']:
            print("\nAu revoir ! üëã")
            break
            
        if not user_input:
            continue
            
        print("\nü§î G√©n√©ration de la r√©ponse...")
        response = generate_response(user_input)
        print("\nü§ñ R√©ponse :")
        print(response)
        print("\n" + "=" * 50)
        
    except Exception as e:
        print(f"\n‚ùå Erreur: {str(e)}")
        print("Veuillez r√©essayer avec une autre question.")
